{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    _1|   _2|\n",
      "+------+-----+\n",
      "|  Java|10000|\n",
      "|Python|10000|\n",
      "| Scala| 5000|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(\"Java\",\"10000\"),(\"Python\",\"10000\"),(\"Scala\",\"5000\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transRDD = spark.sparkContext.textFile(\"trans.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00000000,06-26-2011,4000001,040.33,Exercise & Fitness,Cardio Machine Accessories,Clarksville,Tennessee,credit',\n",
       " '00000001,05-26-2011,4000002,198.44,Exercise & Fitness,Weightlifting Gloves,Long Beach,California,credit',\n",
       " '00000002,06-01-2011,4000002,005.58,Exercise & Fitness,Weightlifting Machine Accessories,Anaheim,California,credit',\n",
       " '00000003,06-05-2011,4000003,198.19,Gymnastics,Gymnastics Rings,Milwaukee,Wisconsin,credit',\n",
       " '00000004,12-17-2011,4000002,098.81,Team Sports,Field Hockey,Nashville  ,Tennessee,credit',\n",
       " '00000005,02-14-2011,4000004,193.63,Outdoor Recreation,Camping & Backpacking & Hiking,Chicago,Illinois,credit',\n",
       " '00000006,10-28-2011,4000005,027.89,Puzzles,Jigsaw Puzzles,Charleston,South Carolina,credit',\n",
       " '00000007,07-14-2011,4000006,096.01,Outdoor Play Equipment,Sandboxes,Columbus,Ohio,credit',\n",
       " '00000008,01-17-2011,4000006,010.44,Winter Sports,Snowmobiling,Des Moines,Iowa,credit',\n",
       " '00000009,05-17-2011,4000006,152.46,Jumping,Bungee Jumping,St. Petersburg,Florida,credit',\n",
       " '00000010,05-29-2011,4000007,180.28,Outdoor Recreation,Archery,Reno,Nevada,credit',\n",
       " '00000011,06-18-2011,4000009,121.39,Outdoor Play Equipment,Swing Sets,Columbus,Ohio,credit',\n",
       " '00000012,02-08-2011,4000009,041.52,Indoor Games,Bowling,San Francisco,California,credit',\n",
       " '00000013,03-13-2011,4000010,107.80,Team Sports,Field Hockey,Honolulu  ,Hawaii,credit',\n",
       " '00000014,02-25-2011,4000010,036.81,Gymnastics,Vaulting Horses,Los Angeles,California,credit',\n",
       " '00000015,10-20-2011,4000001,137.64,Combat Sports,Fencing,Honolulu  ,Hawaii,credit',\n",
       " '00000016,05-28-2011,4000010,035.56,Exercise & Fitness,Free Weight Bars,Columbia,South Carolina,credit',\n",
       " '00000017,10-18-2011,4000008,075.55,Water Sports,Scuba Diving & Snorkeling,Omaha,Nebraska,credit',\n",
       " '00000018,11-18-2011,4000008,088.65,Team Sports,Baseball,Salt Lake City,Utah,credit',\n",
       " '00000019,08-28-2011,4000008,051.81,Water Sports,Life Jackets,Newark,New Jersey,credit',\n",
       " '00000020,06-29-2011,4000005,041.55,Exercise & Fitness,Weightlifting Belts,New Orleans,Louisiana,credit',\n",
       " '00000021,02-14-2011,4000005,045.79,Air Sports,Parachutes,New York,New York,credit',\n",
       " '00000022,10-10-2011,4000009,019.64,Water Sports,Kitesurfing,Saint Paul,Minnesota,credit',\n",
       " '00000023,05-02-2011,4000009,099.50,Gymnastics,Gymnastics Rings,Springfield,Illinois,credit',\n",
       " '00000024,06-10-2011,4000003,151.20,Water Sports,Surfing,Plano,Texas,credit',\n",
       " '00000025,10-14-2011,4000009,144.20,Indoor Games,Darts,Phoenix,Arizona,credit',\n",
       " '00000026,10-11-2011,4000009,031.58,Combat Sports,Wrestling,Orange,California,credit',\n",
       " '00000027,09-29-2011,4000010,066.40,Games,Mahjong,Fremont,California,credit',\n",
       " '00000028,05-12-2011,4000008,079.78,Team Sports,Cricket,Lexington,Kentucky,credit',\n",
       " '00000029,06-03-2011,4000001,126.90,Outdoor Recreation,Hunting,Phoenix,Arizona,credit',\n",
       " '00000030,03-14-2011,4000001,047.05,Water Sports,Swimming,Lincoln,Nebraska,credit',\n",
       " '00000031,11-28-2011,4000008,005.03,Games,Dice & Dice Sets,Los Angeles,California,credit',\n",
       " '00000032,01-29-2011,4000008,020.13,Team Sports,Soccer,Springfield,Illinois,credit',\n",
       " '00000033,06-15-2011,4000008,154.15,Outdoor Recreation,Lawn Games,Nashville  ,Tennessee,credit',\n",
       " '00000034,05-06-2011,4000008,098.96,Team Sports,Indoor Volleyball,Atlanta,Georgia,credit',\n",
       " '00000035,04-12-2011,4000008,185.26,Games,Board Games,Centennial,Colorado,credit',\n",
       " '00000036,10-13-2011,4000007,035.66,Team Sports,Football,Saint Paul,Minnesota,credit',\n",
       " '00000037,04-19-2011,4000007,020.20,Outdoor Recreation,Shooting Games,San Diego,California,credit',\n",
       " '00000038,08-05-2011,4000007,150.60,Outdoor Recreation,Camping & Backpacking & Hiking,Hampton  ,Virginia,credit',\n",
       " '00000039,03-12-2011,4000006,174.36,Outdoor Play Equipment,Swing Sets,Pittsburgh,Pennsylvania,credit',\n",
       " '00000040,11-07-2011,4000005,165.10,Team Sports,Cheerleading,Reno,Nevada,credit',\n",
       " '00000041,04-16-2011,4000004,028.11,Indoor Games,Bowling,Westminster,Colorado,cash',\n",
       " '00000042,09-10-2011,4000004,038.52,Outdoor Recreation,Tetherball,Denton,Texas,cash',\n",
       " '00000043,04-22-2011,4000004,032.34,Water Sports,Water Polo,Las Vegas,Nevada,cash',\n",
       " '00000044,09-11-2011,4000001,135.37,Water Sports,Surfing,Seattle,Washington,credit',\n",
       " '00000045,11-27-2011,4000001,090.04,Exercise & Fitness,Abdominal Equipment,Honolulu  ,Hawaii,credit',\n",
       " '00000046,05-27-2011,4000001,052.29,Gymnastics,Vaulting Horses,Cleveland,Ohio,credit',\n",
       " '00000047,10-23-2011,4000008,100.10,Outdoor Play Equipment,Swing Sets,Everett,Washington,credit',\n",
       " '00000048,09-27-2011,4000007,157.94,Exercise & Fitness,Exercise Bands,Philadelphia,Pennsylvania,credit',\n",
       " '00000049,07-12-2011,4000010,144.59,Jumping,Jumping Stilts,Cambridge,Massachusetts,credit',\n",
       " '00000050,10-20-2011,4000010,055.93,Jumping,Pogo Sticks,Everett,Washington,credit',\n",
       " '00000051,02-17-2011,4000002,032.65,Water Sports,Life Jackets,Columbus,Georgia,cash',\n",
       " '00000052,02-04-2011,4000005,044.82,Outdoor Play Equipment,Lawn Water Slides,Hampton  ,Virginia,cash',\n",
       " '00000053,06-12-2011,4000004,044.46,Water Sports,Scuba Diving & Snorkeling,Charleston,South Carolina,cash',\n",
       " '00000054,10-03-2011,4000007,154.87,Outdoor Recreation,Running,Long Beach,California,credit',\n",
       " '00000055,12-16-2011,4000006,106.11,Water Sports,Swimming,New York,New York,credit',\n",
       " '00000056,06-21-2011,4000002,176.63,Outdoor Recreation,Geocaching,Boston,Massachusetts,credit',\n",
       " '00000057,12-20-2011,4000003,178.20,Outdoor Recreation,Skating,San Jose,California,credit',\n",
       " '00000058,12-29-2011,4000002,194.86,Water Sports,Windsurfing,Oklahoma City,Oklahoma,credit',\n",
       " '00000059,11-07-2011,4000001,021.43,Winter Sports,Snowboarding,Philadelphia,Pennsylvania,cash']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00000000',\n",
       " '06-26-2011',\n",
       " '4000001',\n",
       " '040.33',\n",
       " 'Exercise & Fitness',\n",
       " 'Cardio Machine Accessories',\n",
       " 'Clarksville',\n",
       " 'Tennessee',\n",
       " 'credit',\n",
       " '00000001',\n",
       " '05-26-2011',\n",
       " '4000002',\n",
       " '198.44',\n",
       " 'Exercise & Fitness',\n",
       " 'Weightlifting Gloves',\n",
       " 'Long Beach',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000002',\n",
       " '06-01-2011',\n",
       " '4000002',\n",
       " '005.58',\n",
       " 'Exercise & Fitness',\n",
       " 'Weightlifting Machine Accessories',\n",
       " 'Anaheim',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000003',\n",
       " '06-05-2011',\n",
       " '4000003',\n",
       " '198.19',\n",
       " 'Gymnastics',\n",
       " 'Gymnastics Rings',\n",
       " 'Milwaukee',\n",
       " 'Wisconsin',\n",
       " 'credit',\n",
       " '00000004',\n",
       " '12-17-2011',\n",
       " '4000002',\n",
       " '098.81',\n",
       " 'Team Sports',\n",
       " 'Field Hockey',\n",
       " 'Nashville  ',\n",
       " 'Tennessee',\n",
       " 'credit',\n",
       " '00000005',\n",
       " '02-14-2011',\n",
       " '4000004',\n",
       " '193.63',\n",
       " 'Outdoor Recreation',\n",
       " 'Camping & Backpacking & Hiking',\n",
       " 'Chicago',\n",
       " 'Illinois',\n",
       " 'credit',\n",
       " '00000006',\n",
       " '10-28-2011',\n",
       " '4000005',\n",
       " '027.89',\n",
       " 'Puzzles',\n",
       " 'Jigsaw Puzzles',\n",
       " 'Charleston',\n",
       " 'South Carolina',\n",
       " 'credit',\n",
       " '00000007',\n",
       " '07-14-2011',\n",
       " '4000006',\n",
       " '096.01',\n",
       " 'Outdoor Play Equipment',\n",
       " 'Sandboxes',\n",
       " 'Columbus',\n",
       " 'Ohio',\n",
       " 'credit',\n",
       " '00000008',\n",
       " '01-17-2011',\n",
       " '4000006',\n",
       " '010.44',\n",
       " 'Winter Sports',\n",
       " 'Snowmobiling',\n",
       " 'Des Moines',\n",
       " 'Iowa',\n",
       " 'credit',\n",
       " '00000009',\n",
       " '05-17-2011',\n",
       " '4000006',\n",
       " '152.46',\n",
       " 'Jumping',\n",
       " 'Bungee Jumping',\n",
       " 'St. Petersburg',\n",
       " 'Florida',\n",
       " 'credit',\n",
       " '00000010',\n",
       " '05-29-2011',\n",
       " '4000007',\n",
       " '180.28',\n",
       " 'Outdoor Recreation',\n",
       " 'Archery',\n",
       " 'Reno',\n",
       " 'Nevada',\n",
       " 'credit',\n",
       " '00000011',\n",
       " '06-18-2011',\n",
       " '4000009',\n",
       " '121.39',\n",
       " 'Outdoor Play Equipment',\n",
       " 'Swing Sets',\n",
       " 'Columbus',\n",
       " 'Ohio',\n",
       " 'credit',\n",
       " '00000012',\n",
       " '02-08-2011',\n",
       " '4000009',\n",
       " '041.52',\n",
       " 'Indoor Games',\n",
       " 'Bowling',\n",
       " 'San Francisco',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000013',\n",
       " '03-13-2011',\n",
       " '4000010',\n",
       " '107.80',\n",
       " 'Team Sports',\n",
       " 'Field Hockey',\n",
       " 'Honolulu  ',\n",
       " 'Hawaii',\n",
       " 'credit',\n",
       " '00000014',\n",
       " '02-25-2011',\n",
       " '4000010',\n",
       " '036.81',\n",
       " 'Gymnastics',\n",
       " 'Vaulting Horses',\n",
       " 'Los Angeles',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000015',\n",
       " '10-20-2011',\n",
       " '4000001',\n",
       " '137.64',\n",
       " 'Combat Sports',\n",
       " 'Fencing',\n",
       " 'Honolulu  ',\n",
       " 'Hawaii',\n",
       " 'credit',\n",
       " '00000016',\n",
       " '05-28-2011',\n",
       " '4000010',\n",
       " '035.56',\n",
       " 'Exercise & Fitness',\n",
       " 'Free Weight Bars',\n",
       " 'Columbia',\n",
       " 'South Carolina',\n",
       " 'credit',\n",
       " '00000017',\n",
       " '10-18-2011',\n",
       " '4000008',\n",
       " '075.55',\n",
       " 'Water Sports',\n",
       " 'Scuba Diving & Snorkeling',\n",
       " 'Omaha',\n",
       " 'Nebraska',\n",
       " 'credit',\n",
       " '00000018',\n",
       " '11-18-2011',\n",
       " '4000008',\n",
       " '088.65',\n",
       " 'Team Sports',\n",
       " 'Baseball',\n",
       " 'Salt Lake City',\n",
       " 'Utah',\n",
       " 'credit',\n",
       " '00000019',\n",
       " '08-28-2011',\n",
       " '4000008',\n",
       " '051.81',\n",
       " 'Water Sports',\n",
       " 'Life Jackets',\n",
       " 'Newark',\n",
       " 'New Jersey',\n",
       " 'credit',\n",
       " '00000020',\n",
       " '06-29-2011',\n",
       " '4000005',\n",
       " '041.55',\n",
       " 'Exercise & Fitness',\n",
       " 'Weightlifting Belts',\n",
       " 'New Orleans',\n",
       " 'Louisiana',\n",
       " 'credit',\n",
       " '00000021',\n",
       " '02-14-2011',\n",
       " '4000005',\n",
       " '045.79',\n",
       " 'Air Sports',\n",
       " 'Parachutes',\n",
       " 'New York',\n",
       " 'New York',\n",
       " 'credit',\n",
       " '00000022',\n",
       " '10-10-2011',\n",
       " '4000009',\n",
       " '019.64',\n",
       " 'Water Sports',\n",
       " 'Kitesurfing',\n",
       " 'Saint Paul',\n",
       " 'Minnesota',\n",
       " 'credit',\n",
       " '00000023',\n",
       " '05-02-2011',\n",
       " '4000009',\n",
       " '099.50',\n",
       " 'Gymnastics',\n",
       " 'Gymnastics Rings',\n",
       " 'Springfield',\n",
       " 'Illinois',\n",
       " 'credit',\n",
       " '00000024',\n",
       " '06-10-2011',\n",
       " '4000003',\n",
       " '151.20',\n",
       " 'Water Sports',\n",
       " 'Surfing',\n",
       " 'Plano',\n",
       " 'Texas',\n",
       " 'credit',\n",
       " '00000025',\n",
       " '10-14-2011',\n",
       " '4000009',\n",
       " '144.20',\n",
       " 'Indoor Games',\n",
       " 'Darts',\n",
       " 'Phoenix',\n",
       " 'Arizona',\n",
       " 'credit',\n",
       " '00000026',\n",
       " '10-11-2011',\n",
       " '4000009',\n",
       " '031.58',\n",
       " 'Combat Sports',\n",
       " 'Wrestling',\n",
       " 'Orange',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000027',\n",
       " '09-29-2011',\n",
       " '4000010',\n",
       " '066.40',\n",
       " 'Games',\n",
       " 'Mahjong',\n",
       " 'Fremont',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000028',\n",
       " '05-12-2011',\n",
       " '4000008',\n",
       " '079.78',\n",
       " 'Team Sports',\n",
       " 'Cricket',\n",
       " 'Lexington',\n",
       " 'Kentucky',\n",
       " 'credit',\n",
       " '00000029',\n",
       " '06-03-2011',\n",
       " '4000001',\n",
       " '126.90',\n",
       " 'Outdoor Recreation',\n",
       " 'Hunting',\n",
       " 'Phoenix',\n",
       " 'Arizona',\n",
       " 'credit',\n",
       " '00000030',\n",
       " '03-14-2011',\n",
       " '4000001',\n",
       " '047.05',\n",
       " 'Water Sports',\n",
       " 'Swimming',\n",
       " 'Lincoln',\n",
       " 'Nebraska',\n",
       " 'credit',\n",
       " '00000031',\n",
       " '11-28-2011',\n",
       " '4000008',\n",
       " '005.03',\n",
       " 'Games',\n",
       " 'Dice & Dice Sets',\n",
       " 'Los Angeles',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000032',\n",
       " '01-29-2011',\n",
       " '4000008',\n",
       " '020.13',\n",
       " 'Team Sports',\n",
       " 'Soccer',\n",
       " 'Springfield',\n",
       " 'Illinois',\n",
       " 'credit',\n",
       " '00000033',\n",
       " '06-15-2011',\n",
       " '4000008',\n",
       " '154.15',\n",
       " 'Outdoor Recreation',\n",
       " 'Lawn Games',\n",
       " 'Nashville  ',\n",
       " 'Tennessee',\n",
       " 'credit',\n",
       " '00000034',\n",
       " '05-06-2011',\n",
       " '4000008',\n",
       " '098.96',\n",
       " 'Team Sports',\n",
       " 'Indoor Volleyball',\n",
       " 'Atlanta',\n",
       " 'Georgia',\n",
       " 'credit',\n",
       " '00000035',\n",
       " '04-12-2011',\n",
       " '4000008',\n",
       " '185.26',\n",
       " 'Games',\n",
       " 'Board Games',\n",
       " 'Centennial',\n",
       " 'Colorado',\n",
       " 'credit',\n",
       " '00000036',\n",
       " '10-13-2011',\n",
       " '4000007',\n",
       " '035.66',\n",
       " 'Team Sports',\n",
       " 'Football',\n",
       " 'Saint Paul',\n",
       " 'Minnesota',\n",
       " 'credit',\n",
       " '00000037',\n",
       " '04-19-2011',\n",
       " '4000007',\n",
       " '020.20',\n",
       " 'Outdoor Recreation',\n",
       " 'Shooting Games',\n",
       " 'San Diego',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000038',\n",
       " '08-05-2011',\n",
       " '4000007',\n",
       " '150.60',\n",
       " 'Outdoor Recreation',\n",
       " 'Camping & Backpacking & Hiking',\n",
       " 'Hampton  ',\n",
       " 'Virginia',\n",
       " 'credit',\n",
       " '00000039',\n",
       " '03-12-2011',\n",
       " '4000006',\n",
       " '174.36',\n",
       " 'Outdoor Play Equipment',\n",
       " 'Swing Sets',\n",
       " 'Pittsburgh',\n",
       " 'Pennsylvania',\n",
       " 'credit',\n",
       " '00000040',\n",
       " '11-07-2011',\n",
       " '4000005',\n",
       " '165.10',\n",
       " 'Team Sports',\n",
       " 'Cheerleading',\n",
       " 'Reno',\n",
       " 'Nevada',\n",
       " 'credit',\n",
       " '00000041',\n",
       " '04-16-2011',\n",
       " '4000004',\n",
       " '028.11',\n",
       " 'Indoor Games',\n",
       " 'Bowling',\n",
       " 'Westminster',\n",
       " 'Colorado',\n",
       " 'cash',\n",
       " '00000042',\n",
       " '09-10-2011',\n",
       " '4000004',\n",
       " '038.52',\n",
       " 'Outdoor Recreation',\n",
       " 'Tetherball',\n",
       " 'Denton',\n",
       " 'Texas',\n",
       " 'cash',\n",
       " '00000043',\n",
       " '04-22-2011',\n",
       " '4000004',\n",
       " '032.34',\n",
       " 'Water Sports',\n",
       " 'Water Polo',\n",
       " 'Las Vegas',\n",
       " 'Nevada',\n",
       " 'cash',\n",
       " '00000044',\n",
       " '09-11-2011',\n",
       " '4000001',\n",
       " '135.37',\n",
       " 'Water Sports',\n",
       " 'Surfing',\n",
       " 'Seattle',\n",
       " 'Washington',\n",
       " 'credit',\n",
       " '00000045',\n",
       " '11-27-2011',\n",
       " '4000001',\n",
       " '090.04',\n",
       " 'Exercise & Fitness',\n",
       " 'Abdominal Equipment',\n",
       " 'Honolulu  ',\n",
       " 'Hawaii',\n",
       " 'credit',\n",
       " '00000046',\n",
       " '05-27-2011',\n",
       " '4000001',\n",
       " '052.29',\n",
       " 'Gymnastics',\n",
       " 'Vaulting Horses',\n",
       " 'Cleveland',\n",
       " 'Ohio',\n",
       " 'credit',\n",
       " '00000047',\n",
       " '10-23-2011',\n",
       " '4000008',\n",
       " '100.10',\n",
       " 'Outdoor Play Equipment',\n",
       " 'Swing Sets',\n",
       " 'Everett',\n",
       " 'Washington',\n",
       " 'credit',\n",
       " '00000048',\n",
       " '09-27-2011',\n",
       " '4000007',\n",
       " '157.94',\n",
       " 'Exercise & Fitness',\n",
       " 'Exercise Bands',\n",
       " 'Philadelphia',\n",
       " 'Pennsylvania',\n",
       " 'credit',\n",
       " '00000049',\n",
       " '07-12-2011',\n",
       " '4000010',\n",
       " '144.59',\n",
       " 'Jumping',\n",
       " 'Jumping Stilts',\n",
       " 'Cambridge',\n",
       " 'Massachusetts',\n",
       " 'credit',\n",
       " '00000050',\n",
       " '10-20-2011',\n",
       " '4000010',\n",
       " '055.93',\n",
       " 'Jumping',\n",
       " 'Pogo Sticks',\n",
       " 'Everett',\n",
       " 'Washington',\n",
       " 'credit',\n",
       " '00000051',\n",
       " '02-17-2011',\n",
       " '4000002',\n",
       " '032.65',\n",
       " 'Water Sports',\n",
       " 'Life Jackets',\n",
       " 'Columbus',\n",
       " 'Georgia',\n",
       " 'cash',\n",
       " '00000052',\n",
       " '02-04-2011',\n",
       " '4000005',\n",
       " '044.82',\n",
       " 'Outdoor Play Equipment',\n",
       " 'Lawn Water Slides',\n",
       " 'Hampton  ',\n",
       " 'Virginia',\n",
       " 'cash',\n",
       " '00000053',\n",
       " '06-12-2011',\n",
       " '4000004',\n",
       " '044.46',\n",
       " 'Water Sports',\n",
       " 'Scuba Diving & Snorkeling',\n",
       " 'Charleston',\n",
       " 'South Carolina',\n",
       " 'cash',\n",
       " '00000054',\n",
       " '10-03-2011',\n",
       " '4000007',\n",
       " '154.87',\n",
       " 'Outdoor Recreation',\n",
       " 'Running',\n",
       " 'Long Beach',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000055',\n",
       " '12-16-2011',\n",
       " '4000006',\n",
       " '106.11',\n",
       " 'Water Sports',\n",
       " 'Swimming',\n",
       " 'New York',\n",
       " 'New York',\n",
       " 'credit',\n",
       " '00000056',\n",
       " '06-21-2011',\n",
       " '4000002',\n",
       " '176.63',\n",
       " 'Outdoor Recreation',\n",
       " 'Geocaching',\n",
       " 'Boston',\n",
       " 'Massachusetts',\n",
       " 'credit',\n",
       " '00000057',\n",
       " '12-20-2011',\n",
       " '4000003',\n",
       " '178.20',\n",
       " 'Outdoor Recreation',\n",
       " 'Skating',\n",
       " 'San Jose',\n",
       " 'California',\n",
       " 'credit',\n",
       " '00000058',\n",
       " '12-29-2011',\n",
       " '4000002',\n",
       " '194.86',\n",
       " 'Water Sports',\n",
       " 'Windsurfing',\n",
       " 'Oklahoma City',\n",
       " 'Oklahoma',\n",
       " 'credit',\n",
       " '00000059',\n",
       " '11-07-2011',\n",
       " '4000001',\n",
       " '021.43',\n",
       " 'Winter Sports',\n",
       " 'Snowboarding',\n",
       " 'Philadelphia',\n",
       " 'Pennsylvania',\n",
       " 'cash']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.flatMap(lambda y: y.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['00000000',\n",
       "  '06-26-2011',\n",
       "  '4000001',\n",
       "  '040.33',\n",
       "  'Exercise & Fitness',\n",
       "  'Cardio Machine Accessories',\n",
       "  'Clarksville',\n",
       "  'Tennessee',\n",
       "  'credit'],\n",
       " ['00000001',\n",
       "  '05-26-2011',\n",
       "  '4000002',\n",
       "  '198.44',\n",
       "  'Exercise & Fitness',\n",
       "  'Weightlifting Gloves',\n",
       "  'Long Beach',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000002',\n",
       "  '06-01-2011',\n",
       "  '4000002',\n",
       "  '005.58',\n",
       "  'Exercise & Fitness',\n",
       "  'Weightlifting Machine Accessories',\n",
       "  'Anaheim',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000003',\n",
       "  '06-05-2011',\n",
       "  '4000003',\n",
       "  '198.19',\n",
       "  'Gymnastics',\n",
       "  'Gymnastics Rings',\n",
       "  'Milwaukee',\n",
       "  'Wisconsin',\n",
       "  'credit'],\n",
       " ['00000004',\n",
       "  '12-17-2011',\n",
       "  '4000002',\n",
       "  '098.81',\n",
       "  'Team Sports',\n",
       "  'Field Hockey',\n",
       "  'Nashville  ',\n",
       "  'Tennessee',\n",
       "  'credit'],\n",
       " ['00000005',\n",
       "  '02-14-2011',\n",
       "  '4000004',\n",
       "  '193.63',\n",
       "  'Outdoor Recreation',\n",
       "  'Camping & Backpacking & Hiking',\n",
       "  'Chicago',\n",
       "  'Illinois',\n",
       "  'credit'],\n",
       " ['00000006',\n",
       "  '10-28-2011',\n",
       "  '4000005',\n",
       "  '027.89',\n",
       "  'Puzzles',\n",
       "  'Jigsaw Puzzles',\n",
       "  'Charleston',\n",
       "  'South Carolina',\n",
       "  'credit'],\n",
       " ['00000007',\n",
       "  '07-14-2011',\n",
       "  '4000006',\n",
       "  '096.01',\n",
       "  'Outdoor Play Equipment',\n",
       "  'Sandboxes',\n",
       "  'Columbus',\n",
       "  'Ohio',\n",
       "  'credit'],\n",
       " ['00000008',\n",
       "  '01-17-2011',\n",
       "  '4000006',\n",
       "  '010.44',\n",
       "  'Winter Sports',\n",
       "  'Snowmobiling',\n",
       "  'Des Moines',\n",
       "  'Iowa',\n",
       "  'credit'],\n",
       " ['00000009',\n",
       "  '05-17-2011',\n",
       "  '4000006',\n",
       "  '152.46',\n",
       "  'Jumping',\n",
       "  'Bungee Jumping',\n",
       "  'St. Petersburg',\n",
       "  'Florida',\n",
       "  'credit'],\n",
       " ['00000010',\n",
       "  '05-29-2011',\n",
       "  '4000007',\n",
       "  '180.28',\n",
       "  'Outdoor Recreation',\n",
       "  'Archery',\n",
       "  'Reno',\n",
       "  'Nevada',\n",
       "  'credit'],\n",
       " ['00000011',\n",
       "  '06-18-2011',\n",
       "  '4000009',\n",
       "  '121.39',\n",
       "  'Outdoor Play Equipment',\n",
       "  'Swing Sets',\n",
       "  'Columbus',\n",
       "  'Ohio',\n",
       "  'credit'],\n",
       " ['00000012',\n",
       "  '02-08-2011',\n",
       "  '4000009',\n",
       "  '041.52',\n",
       "  'Indoor Games',\n",
       "  'Bowling',\n",
       "  'San Francisco',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000013',\n",
       "  '03-13-2011',\n",
       "  '4000010',\n",
       "  '107.80',\n",
       "  'Team Sports',\n",
       "  'Field Hockey',\n",
       "  'Honolulu  ',\n",
       "  'Hawaii',\n",
       "  'credit'],\n",
       " ['00000014',\n",
       "  '02-25-2011',\n",
       "  '4000010',\n",
       "  '036.81',\n",
       "  'Gymnastics',\n",
       "  'Vaulting Horses',\n",
       "  'Los Angeles',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000015',\n",
       "  '10-20-2011',\n",
       "  '4000001',\n",
       "  '137.64',\n",
       "  'Combat Sports',\n",
       "  'Fencing',\n",
       "  'Honolulu  ',\n",
       "  'Hawaii',\n",
       "  'credit'],\n",
       " ['00000016',\n",
       "  '05-28-2011',\n",
       "  '4000010',\n",
       "  '035.56',\n",
       "  'Exercise & Fitness',\n",
       "  'Free Weight Bars',\n",
       "  'Columbia',\n",
       "  'South Carolina',\n",
       "  'credit'],\n",
       " ['00000017',\n",
       "  '10-18-2011',\n",
       "  '4000008',\n",
       "  '075.55',\n",
       "  'Water Sports',\n",
       "  'Scuba Diving & Snorkeling',\n",
       "  'Omaha',\n",
       "  'Nebraska',\n",
       "  'credit'],\n",
       " ['00000018',\n",
       "  '11-18-2011',\n",
       "  '4000008',\n",
       "  '088.65',\n",
       "  'Team Sports',\n",
       "  'Baseball',\n",
       "  'Salt Lake City',\n",
       "  'Utah',\n",
       "  'credit'],\n",
       " ['00000019',\n",
       "  '08-28-2011',\n",
       "  '4000008',\n",
       "  '051.81',\n",
       "  'Water Sports',\n",
       "  'Life Jackets',\n",
       "  'Newark',\n",
       "  'New Jersey',\n",
       "  'credit'],\n",
       " ['00000020',\n",
       "  '06-29-2011',\n",
       "  '4000005',\n",
       "  '041.55',\n",
       "  'Exercise & Fitness',\n",
       "  'Weightlifting Belts',\n",
       "  'New Orleans',\n",
       "  'Louisiana',\n",
       "  'credit'],\n",
       " ['00000021',\n",
       "  '02-14-2011',\n",
       "  '4000005',\n",
       "  '045.79',\n",
       "  'Air Sports',\n",
       "  'Parachutes',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'credit'],\n",
       " ['00000022',\n",
       "  '10-10-2011',\n",
       "  '4000009',\n",
       "  '019.64',\n",
       "  'Water Sports',\n",
       "  'Kitesurfing',\n",
       "  'Saint Paul',\n",
       "  'Minnesota',\n",
       "  'credit'],\n",
       " ['00000023',\n",
       "  '05-02-2011',\n",
       "  '4000009',\n",
       "  '099.50',\n",
       "  'Gymnastics',\n",
       "  'Gymnastics Rings',\n",
       "  'Springfield',\n",
       "  'Illinois',\n",
       "  'credit'],\n",
       " ['00000024',\n",
       "  '06-10-2011',\n",
       "  '4000003',\n",
       "  '151.20',\n",
       "  'Water Sports',\n",
       "  'Surfing',\n",
       "  'Plano',\n",
       "  'Texas',\n",
       "  'credit'],\n",
       " ['00000025',\n",
       "  '10-14-2011',\n",
       "  '4000009',\n",
       "  '144.20',\n",
       "  'Indoor Games',\n",
       "  'Darts',\n",
       "  'Phoenix',\n",
       "  'Arizona',\n",
       "  'credit'],\n",
       " ['00000026',\n",
       "  '10-11-2011',\n",
       "  '4000009',\n",
       "  '031.58',\n",
       "  'Combat Sports',\n",
       "  'Wrestling',\n",
       "  'Orange',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000027',\n",
       "  '09-29-2011',\n",
       "  '4000010',\n",
       "  '066.40',\n",
       "  'Games',\n",
       "  'Mahjong',\n",
       "  'Fremont',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000028',\n",
       "  '05-12-2011',\n",
       "  '4000008',\n",
       "  '079.78',\n",
       "  'Team Sports',\n",
       "  'Cricket',\n",
       "  'Lexington',\n",
       "  'Kentucky',\n",
       "  'credit'],\n",
       " ['00000029',\n",
       "  '06-03-2011',\n",
       "  '4000001',\n",
       "  '126.90',\n",
       "  'Outdoor Recreation',\n",
       "  'Hunting',\n",
       "  'Phoenix',\n",
       "  'Arizona',\n",
       "  'credit'],\n",
       " ['00000030',\n",
       "  '03-14-2011',\n",
       "  '4000001',\n",
       "  '047.05',\n",
       "  'Water Sports',\n",
       "  'Swimming',\n",
       "  'Lincoln',\n",
       "  'Nebraska',\n",
       "  'credit'],\n",
       " ['00000031',\n",
       "  '11-28-2011',\n",
       "  '4000008',\n",
       "  '005.03',\n",
       "  'Games',\n",
       "  'Dice & Dice Sets',\n",
       "  'Los Angeles',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000032',\n",
       "  '01-29-2011',\n",
       "  '4000008',\n",
       "  '020.13',\n",
       "  'Team Sports',\n",
       "  'Soccer',\n",
       "  'Springfield',\n",
       "  'Illinois',\n",
       "  'credit'],\n",
       " ['00000033',\n",
       "  '06-15-2011',\n",
       "  '4000008',\n",
       "  '154.15',\n",
       "  'Outdoor Recreation',\n",
       "  'Lawn Games',\n",
       "  'Nashville  ',\n",
       "  'Tennessee',\n",
       "  'credit'],\n",
       " ['00000034',\n",
       "  '05-06-2011',\n",
       "  '4000008',\n",
       "  '098.96',\n",
       "  'Team Sports',\n",
       "  'Indoor Volleyball',\n",
       "  'Atlanta',\n",
       "  'Georgia',\n",
       "  'credit'],\n",
       " ['00000035',\n",
       "  '04-12-2011',\n",
       "  '4000008',\n",
       "  '185.26',\n",
       "  'Games',\n",
       "  'Board Games',\n",
       "  'Centennial',\n",
       "  'Colorado',\n",
       "  'credit'],\n",
       " ['00000036',\n",
       "  '10-13-2011',\n",
       "  '4000007',\n",
       "  '035.66',\n",
       "  'Team Sports',\n",
       "  'Football',\n",
       "  'Saint Paul',\n",
       "  'Minnesota',\n",
       "  'credit'],\n",
       " ['00000037',\n",
       "  '04-19-2011',\n",
       "  '4000007',\n",
       "  '020.20',\n",
       "  'Outdoor Recreation',\n",
       "  'Shooting Games',\n",
       "  'San Diego',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000038',\n",
       "  '08-05-2011',\n",
       "  '4000007',\n",
       "  '150.60',\n",
       "  'Outdoor Recreation',\n",
       "  'Camping & Backpacking & Hiking',\n",
       "  'Hampton  ',\n",
       "  'Virginia',\n",
       "  'credit'],\n",
       " ['00000039',\n",
       "  '03-12-2011',\n",
       "  '4000006',\n",
       "  '174.36',\n",
       "  'Outdoor Play Equipment',\n",
       "  'Swing Sets',\n",
       "  'Pittsburgh',\n",
       "  'Pennsylvania',\n",
       "  'credit'],\n",
       " ['00000040',\n",
       "  '11-07-2011',\n",
       "  '4000005',\n",
       "  '165.10',\n",
       "  'Team Sports',\n",
       "  'Cheerleading',\n",
       "  'Reno',\n",
       "  'Nevada',\n",
       "  'credit'],\n",
       " ['00000041',\n",
       "  '04-16-2011',\n",
       "  '4000004',\n",
       "  '028.11',\n",
       "  'Indoor Games',\n",
       "  'Bowling',\n",
       "  'Westminster',\n",
       "  'Colorado',\n",
       "  'cash'],\n",
       " ['00000042',\n",
       "  '09-10-2011',\n",
       "  '4000004',\n",
       "  '038.52',\n",
       "  'Outdoor Recreation',\n",
       "  'Tetherball',\n",
       "  'Denton',\n",
       "  'Texas',\n",
       "  'cash'],\n",
       " ['00000043',\n",
       "  '04-22-2011',\n",
       "  '4000004',\n",
       "  '032.34',\n",
       "  'Water Sports',\n",
       "  'Water Polo',\n",
       "  'Las Vegas',\n",
       "  'Nevada',\n",
       "  'cash'],\n",
       " ['00000044',\n",
       "  '09-11-2011',\n",
       "  '4000001',\n",
       "  '135.37',\n",
       "  'Water Sports',\n",
       "  'Surfing',\n",
       "  'Seattle',\n",
       "  'Washington',\n",
       "  'credit'],\n",
       " ['00000045',\n",
       "  '11-27-2011',\n",
       "  '4000001',\n",
       "  '090.04',\n",
       "  'Exercise & Fitness',\n",
       "  'Abdominal Equipment',\n",
       "  'Honolulu  ',\n",
       "  'Hawaii',\n",
       "  'credit'],\n",
       " ['00000046',\n",
       "  '05-27-2011',\n",
       "  '4000001',\n",
       "  '052.29',\n",
       "  'Gymnastics',\n",
       "  'Vaulting Horses',\n",
       "  'Cleveland',\n",
       "  'Ohio',\n",
       "  'credit'],\n",
       " ['00000047',\n",
       "  '10-23-2011',\n",
       "  '4000008',\n",
       "  '100.10',\n",
       "  'Outdoor Play Equipment',\n",
       "  'Swing Sets',\n",
       "  'Everett',\n",
       "  'Washington',\n",
       "  'credit'],\n",
       " ['00000048',\n",
       "  '09-27-2011',\n",
       "  '4000007',\n",
       "  '157.94',\n",
       "  'Exercise & Fitness',\n",
       "  'Exercise Bands',\n",
       "  'Philadelphia',\n",
       "  'Pennsylvania',\n",
       "  'credit'],\n",
       " ['00000049',\n",
       "  '07-12-2011',\n",
       "  '4000010',\n",
       "  '144.59',\n",
       "  'Jumping',\n",
       "  'Jumping Stilts',\n",
       "  'Cambridge',\n",
       "  'Massachusetts',\n",
       "  'credit'],\n",
       " ['00000050',\n",
       "  '10-20-2011',\n",
       "  '4000010',\n",
       "  '055.93',\n",
       "  'Jumping',\n",
       "  'Pogo Sticks',\n",
       "  'Everett',\n",
       "  'Washington',\n",
       "  'credit'],\n",
       " ['00000051',\n",
       "  '02-17-2011',\n",
       "  '4000002',\n",
       "  '032.65',\n",
       "  'Water Sports',\n",
       "  'Life Jackets',\n",
       "  'Columbus',\n",
       "  'Georgia',\n",
       "  'cash'],\n",
       " ['00000052',\n",
       "  '02-04-2011',\n",
       "  '4000005',\n",
       "  '044.82',\n",
       "  'Outdoor Play Equipment',\n",
       "  'Lawn Water Slides',\n",
       "  'Hampton  ',\n",
       "  'Virginia',\n",
       "  'cash'],\n",
       " ['00000053',\n",
       "  '06-12-2011',\n",
       "  '4000004',\n",
       "  '044.46',\n",
       "  'Water Sports',\n",
       "  'Scuba Diving & Snorkeling',\n",
       "  'Charleston',\n",
       "  'South Carolina',\n",
       "  'cash'],\n",
       " ['00000054',\n",
       "  '10-03-2011',\n",
       "  '4000007',\n",
       "  '154.87',\n",
       "  'Outdoor Recreation',\n",
       "  'Running',\n",
       "  'Long Beach',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000055',\n",
       "  '12-16-2011',\n",
       "  '4000006',\n",
       "  '106.11',\n",
       "  'Water Sports',\n",
       "  'Swimming',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'credit'],\n",
       " ['00000056',\n",
       "  '06-21-2011',\n",
       "  '4000002',\n",
       "  '176.63',\n",
       "  'Outdoor Recreation',\n",
       "  'Geocaching',\n",
       "  'Boston',\n",
       "  'Massachusetts',\n",
       "  'credit'],\n",
       " ['00000057',\n",
       "  '12-20-2011',\n",
       "  '4000003',\n",
       "  '178.20',\n",
       "  'Outdoor Recreation',\n",
       "  'Skating',\n",
       "  'San Jose',\n",
       "  'California',\n",
       "  'credit'],\n",
       " ['00000058',\n",
       "  '12-29-2011',\n",
       "  '4000002',\n",
       "  '194.86',\n",
       "  'Water Sports',\n",
       "  'Windsurfing',\n",
       "  'Oklahoma City',\n",
       "  'Oklahoma',\n",
       "  'credit'],\n",
       " ['00000059',\n",
       "  '11-07-2011',\n",
       "  '4000001',\n",
       "  '021.43',\n",
       "  'Winter Sports',\n",
       "  'Snowboarding',\n",
       "  'Philadelphia',\n",
       "  'Pennsylvania',\n",
       "  'cash']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.flatMap(lambda x: x.split(\",\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000001', '040.33'),\n",
       " ('4000002', '198.44'),\n",
       " ('4000002', '005.58'),\n",
       " ('4000003', '198.19'),\n",
       " ('4000002', '098.81'),\n",
       " ('4000004', '193.63'),\n",
       " ('4000005', '027.89'),\n",
       " ('4000006', '096.01'),\n",
       " ('4000006', '010.44'),\n",
       " ('4000006', '152.46'),\n",
       " ('4000007', '180.28'),\n",
       " ('4000009', '121.39'),\n",
       " ('4000009', '041.52'),\n",
       " ('4000010', '107.80'),\n",
       " ('4000010', '036.81'),\n",
       " ('4000001', '137.64'),\n",
       " ('4000010', '035.56'),\n",
       " ('4000008', '075.55'),\n",
       " ('4000008', '088.65'),\n",
       " ('4000008', '051.81'),\n",
       " ('4000005', '041.55'),\n",
       " ('4000005', '045.79'),\n",
       " ('4000009', '019.64'),\n",
       " ('4000009', '099.50'),\n",
       " ('4000003', '151.20'),\n",
       " ('4000009', '144.20'),\n",
       " ('4000009', '031.58'),\n",
       " ('4000010', '066.40'),\n",
       " ('4000008', '079.78'),\n",
       " ('4000001', '126.90'),\n",
       " ('4000001', '047.05'),\n",
       " ('4000008', '005.03'),\n",
       " ('4000008', '020.13'),\n",
       " ('4000008', '154.15'),\n",
       " ('4000008', '098.96'),\n",
       " ('4000008', '185.26'),\n",
       " ('4000007', '035.66'),\n",
       " ('4000007', '020.20'),\n",
       " ('4000007', '150.60'),\n",
       " ('4000006', '174.36'),\n",
       " ('4000005', '165.10'),\n",
       " ('4000004', '028.11'),\n",
       " ('4000004', '038.52'),\n",
       " ('4000004', '032.34'),\n",
       " ('4000001', '135.37'),\n",
       " ('4000001', '090.04'),\n",
       " ('4000001', '052.29'),\n",
       " ('4000008', '100.10'),\n",
       " ('4000007', '157.94'),\n",
       " ('4000010', '144.59'),\n",
       " ('4000010', '055.93'),\n",
       " ('4000002', '032.65'),\n",
       " ('4000005', '044.82'),\n",
       " ('4000004', '044.46'),\n",
       " ('4000007', '154.87'),\n",
       " ('4000006', '106.11'),\n",
       " ('4000002', '176.63'),\n",
       " ('4000003', '178.20'),\n",
       " ('4000002', '194.86'),\n",
       " ('4000001', '021.43')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000004', 337.06),\n",
       " ('4000007', 699.55),\n",
       " ('4000008', 859.42),\n",
       " ('4000001', 651.0500000000001),\n",
       " ('4000002', 706.97),\n",
       " ('4000003', 527.5899999999999),\n",
       " ('4000005', 325.15),\n",
       " ('4000006', 539.3800000000001),\n",
       " ('4000009', 457.83),\n",
       " ('4000010', 447.09000000000003)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).reduceByKey(lambda x1,x2: float(x1)+float(x2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000008', 859.42),\n",
       " ('4000002', 706.97),\n",
       " ('4000007', 699.55),\n",
       " ('4000001', 651.0500000000001),\n",
       " ('4000006', 539.3800000000001),\n",
       " ('4000003', 527.5899999999999),\n",
       " ('4000009', 457.83),\n",
       " ('4000010', 447.09000000000003),\n",
       " ('4000004', 337.06),\n",
       " ('4000005', 325.15)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).reduceByKey(lambda x1,x2: float(x1)+float(x2)).sortBy(lambda x: x[1], False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000001', 651.0500000000001),\n",
       " ('4000002', 706.97),\n",
       " ('4000003', 527.5899999999999),\n",
       " ('4000004', 337.06),\n",
       " ('4000005', 325.15),\n",
       " ('4000006', 539.3800000000001),\n",
       " ('4000007', 699.55),\n",
       " ('4000008', 859.42),\n",
       " ('4000009', 457.83),\n",
       " ('4000010', 447.09000000000003)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).reduceByKey(lambda x1,x2: float(x1)+float(x2)).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000002', 'Team Sports'),\n",
       " ('4000006', 'Winter Sports'),\n",
       " ('4000010', 'Team Sports'),\n",
       " ('4000001', 'Combat Sports'),\n",
       " ('4000008', 'Water Sports'),\n",
       " ('4000008', 'Team Sports'),\n",
       " ('4000008', 'Water Sports'),\n",
       " ('4000005', 'Air Sports'),\n",
       " ('4000009', 'Water Sports'),\n",
       " ('4000003', 'Water Sports'),\n",
       " ('4000009', 'Combat Sports'),\n",
       " ('4000008', 'Team Sports'),\n",
       " ('4000001', 'Water Sports'),\n",
       " ('4000008', 'Team Sports'),\n",
       " ('4000008', 'Team Sports'),\n",
       " ('4000007', 'Team Sports'),\n",
       " ('4000005', 'Team Sports'),\n",
       " ('4000004', 'Water Sports'),\n",
       " ('4000001', 'Water Sports'),\n",
       " ('4000002', 'Water Sports'),\n",
       " ('4000004', 'Water Sports'),\n",
       " ('4000006', 'Water Sports'),\n",
       " ('4000002', 'Water Sports'),\n",
       " ('4000001', 'Winter Sports')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[4])).filter(lambda x: (\"Sport\" in x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000008',\n",
       "  'Water Sports;Team Sports;Games;Outdoor Play Equipment;Outdoor Recreation'),\n",
       " ('4000004', 'Indoor Games;Water Sports;Outdoor Recreation'),\n",
       " ('4000003', 'Gymnastics;Outdoor Recreation;Water Sports'),\n",
       " ('4000006', 'Jumping;Outdoor Play Equipment;Winter Sports;Water Sports'),\n",
       " ('4000001',\n",
       "  'Combat Sports;Outdoor Recreation;Gymnastics;Exercise & Fitness;Water Sports;Winter Sports'),\n",
       " ('4000009',\n",
       "  'Gymnastics;Combat Sports;Outdoor Play Equipment;Indoor Games;Water Sports'),\n",
       " ('4000002', 'Outdoor Recreation;Exercise & Fitness;Team Sports;Water Sports')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[4])).distinct().reduceByKey(lambda x1,x2: x1 + (';') + x2).filter(lambda x: (\"Water Sports\" in x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'4000001': 8,\n",
       "             '4000002': 6,\n",
       "             '4000003': 3,\n",
       "             '4000004': 5,\n",
       "             '4000005': 5,\n",
       "             '4000006': 5,\n",
       "             '4000007': 6,\n",
       "             '4000009': 6,\n",
       "             '4000010': 6,\n",
       "             '4000008': 10})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'4000001': 8,\n",
       "             '4000002': 6,\n",
       "             '4000003': 3,\n",
       "             '4000004': 5,\n",
       "             '4000005': 5,\n",
       "             '4000006': 5,\n",
       "             '4000007': 6,\n",
       "             '4000008': 10,\n",
       "             '4000009': 6,\n",
       "             '4000010': 6})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[4])).sortByKey().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000005', 325.15),\n",
       " ('4000004', 337.06),\n",
       " ('4000010', 447.09000000000003),\n",
       " ('4000009', 457.83),\n",
       " ('4000003', 527.5899999999999),\n",
       " ('4000006', 539.3800000000001),\n",
       " ('4000001', 651.0500000000001),\n",
       " ('4000007', 699.55),\n",
       " ('4000002', 706.97),\n",
       " ('4000008', 859.42)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).reduceByKey(lambda x1,x2: float(x1)+float(x2)).sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'collect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d59c410145d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtransRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.defaultdict' object has no attribute 'collect'"
     ]
    }
   ],
   "source": [
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[4])).sortByKey().countByKey().collect()\n",
    "transRDD.map(lambda x: x.split(\",\")).map(lambda x: (x[2],x[3])).reduceByKey(lambda x1,x2: (float(x1)+float(x2))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[247] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "print(transRDDT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\", \"users_Count\"]\n",
    "data = [(\"Java\",\"20000\"),(\"Python\",\"100000\"),(\"Scala\",\"3000\")]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', '20000'), ('Python', '100000'), ('Scala', '3000')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_Count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_Count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_Count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Java\",\"20000\"),(\"Python\",\"100000\"),(\"Scala\",\"3000\")]\n",
    "dfFromRDD2 = spark.createDataFrame(data).toDF(*columns)\n",
    "dfFromRDD2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_Count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rowData = map(lambda x: Row(*x), data)\n",
    "dfFromRDD3 = spark.createDataFrame(rowData, columns)\n",
    "dfFromRDD3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "         (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "         (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "        ]\n",
    "schema1 = StructType([\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\",StringType(),True), \\\n",
    "    StructField(\"gender\",StringType(),True), \\\n",
    "    StructField(\"salary\",IntegerType(),True), \\\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data = data2, schema= schema1)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|         _c0|    _c1|        _c2|                _c3|  _c4|           _c5|  _c6|    _c7|  _c8|  _c9| _c10|       _c11|   _c12|                _c13|                _c14|         _c15|           _c16|               _c17|      _c18|         _c19|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        FALSE|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        FALSE|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        FALSE|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        FALSE|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        FALSE|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        FALSE|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        FALSE|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        FALSE|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        FALSE|            842|               1666|  28876493|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"zipcodes.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|         _c0|    _c1|        _c2|                _c3|  _c4|           _c5|  _c6|    _c7|  _c8|  _c9| _c10|       _c11|   _c12|                _c13|                _c14|         _c15|           _c16|               _c17|      _c18|         _c19|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        FALSE|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        FALSE|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        FALSE|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        FALSE|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        FALSE|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        FALSE|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        FALSE|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        FALSE|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        FALSE|            842|               1666|  28876493|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").load(\"zipcodes.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Xaxis: string (nullable = true)\n",
      " |-- Yaxis: string (nullable = true)\n",
      " |-- Zaxis: string (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: string (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: string (nullable = true)\n",
      " |-- TotalWages: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        FALSE|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        FALSE|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        FALSE|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        FALSE|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        FALSE|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        FALSE|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        FALSE|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        FALSE|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        FALSE|            842|               1666|  28876493|         null|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|35.71| -79.81| 0.14|-0.79| 0.58|         NA|     US|        Asheboro, NC|   NA-US-NC-ASHEBORO|        FALSE|           8355|              15228| 215474318|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"header\",True).csv(\"zipcodes.csv\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|     _c0|       _c1|    _c2|   _c3|                 _c4|                 _c5|           _c6|           _c7|   _c8|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|00000000|06-26-2011|4000001|040.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|00000001|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|00000002|06-01-2011|4000002|005.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|00000003|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|00000004|12-17-2011|4000002|098.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|00000005|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|00000006|10-28-2011|4000005|027.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|00000007|07-14-2011|4000006|096.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|00000008|01-17-2011|4000006|010.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|00000009|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|00000010|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|00000011|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|00000012|02-08-2011|4000009|041.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|00000013|03-13-2011|4000010|107.80|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|00000014|02-25-2011|4000010|036.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|00000015|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|00000016|05-28-2011|4000010|035.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|00000017|10-18-2011|4000008|075.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|00000018|11-18-2011|4000008|088.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|00000019|08-28-2011|4000008|051.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "|00000020|06-29-2011|4000005|041.55|  Exercise & Fitness| Weightlifting Belts|   New Orleans|     Louisiana|credit|\n",
      "|00000021|02-14-2011|4000005|045.79|          Air Sports|          Parachutes|      New York|      New York|credit|\n",
      "|00000022|10-10-2011|4000009|019.64|        Water Sports|         Kitesurfing|    Saint Paul|     Minnesota|credit|\n",
      "|00000023|05-02-2011|4000009|099.50|          Gymnastics|    Gymnastics Rings|   Springfield|      Illinois|credit|\n",
      "|00000024|06-10-2011|4000003|151.20|        Water Sports|             Surfing|         Plano|         Texas|credit|\n",
      "|00000025|10-14-2011|4000009|144.20|        Indoor Games|               Darts|       Phoenix|       Arizona|credit|\n",
      "|00000026|10-11-2011|4000009|031.58|       Combat Sports|           Wrestling|        Orange|    California|credit|\n",
      "|00000027|09-29-2011|4000010|066.40|               Games|             Mahjong|       Fremont|    California|credit|\n",
      "|00000028|05-12-2011|4000008|079.78|         Team Sports|             Cricket|     Lexington|      Kentucky|credit|\n",
      "|00000029|06-03-2011|4000001|126.90|  Outdoor Recreation|             Hunting|       Phoenix|       Arizona|credit|\n",
      "|00000030|03-14-2011|4000001|047.05|        Water Sports|            Swimming|       Lincoln|      Nebraska|credit|\n",
      "|00000031|11-28-2011|4000008|005.03|               Games|    Dice & Dice Sets|   Los Angeles|    California|credit|\n",
      "|00000032|01-29-2011|4000008|020.13|         Team Sports|              Soccer|   Springfield|      Illinois|credit|\n",
      "|00000033|06-15-2011|4000008|154.15|  Outdoor Recreation|          Lawn Games|   Nashville  |     Tennessee|credit|\n",
      "|00000034|05-06-2011|4000008|098.96|         Team Sports|   Indoor Volleyball|       Atlanta|       Georgia|credit|\n",
      "|00000035|04-12-2011|4000008|185.26|               Games|         Board Games|    Centennial|      Colorado|credit|\n",
      "|00000036|10-13-2011|4000007|035.66|         Team Sports|            Football|    Saint Paul|     Minnesota|credit|\n",
      "|00000037|04-19-2011|4000007|020.20|  Outdoor Recreation|      Shooting Games|     San Diego|    California|credit|\n",
      "|00000038|08-05-2011|4000007|150.60|  Outdoor Recreation|Camping & Backpac...|     Hampton  |      Virginia|credit|\n",
      "|00000039|03-12-2011|4000006|174.36|Outdoor Play Equi...|          Swing Sets|    Pittsburgh|  Pennsylvania|credit|\n",
      "|00000040|11-07-2011|4000005|165.10|         Team Sports|        Cheerleading|          Reno|        Nevada|credit|\n",
      "|00000041|04-16-2011|4000004|028.11|        Indoor Games|             Bowling|   Westminster|      Colorado|  cash|\n",
      "|00000042|09-10-2011|4000004|038.52|  Outdoor Recreation|          Tetherball|        Denton|         Texas|  cash|\n",
      "|00000043|04-22-2011|4000004|032.34|        Water Sports|          Water Polo|     Las Vegas|        Nevada|  cash|\n",
      "|00000044|09-11-2011|4000001|135.37|        Water Sports|             Surfing|       Seattle|    Washington|credit|\n",
      "|00000045|11-27-2011|4000001|090.04|  Exercise & Fitness| Abdominal Equipment|    Honolulu  |        Hawaii|credit|\n",
      "|00000046|05-27-2011|4000001|052.29|          Gymnastics|     Vaulting Horses|     Cleveland|          Ohio|credit|\n",
      "|00000047|10-23-2011|4000008|100.10|Outdoor Play Equi...|          Swing Sets|       Everett|    Washington|credit|\n",
      "|00000048|09-27-2011|4000007|157.94|  Exercise & Fitness|      Exercise Bands|  Philadelphia|  Pennsylvania|credit|\n",
      "|00000049|07-12-2011|4000010|144.59|             Jumping|      Jumping Stilts|     Cambridge| Massachusetts|credit|\n",
      "|00000050|10-20-2011|4000010|055.93|             Jumping|         Pogo Sticks|       Everett|    Washington|credit|\n",
      "|00000051|02-17-2011|4000002|032.65|        Water Sports|        Life Jackets|      Columbus|       Georgia|  cash|\n",
      "|00000052|02-04-2011|4000005|044.82|Outdoor Play Equi...|   Lawn Water Slides|     Hampton  |      Virginia|  cash|\n",
      "|00000053|06-12-2011|4000004|044.46|        Water Sports|Scuba Diving & Sn...|    Charleston|South Carolina|  cash|\n",
      "|00000054|10-03-2011|4000007|154.87|  Outdoor Recreation|             Running|    Long Beach|    California|credit|\n",
      "|00000055|12-16-2011|4000006|106.11|        Water Sports|            Swimming|      New York|      New York|credit|\n",
      "|00000056|06-21-2011|4000002|176.63|  Outdoor Recreation|          Geocaching|        Boston| Massachusetts|credit|\n",
      "|00000057|12-20-2011|4000003|178.20|  Outdoor Recreation|             Skating|      San Jose|    California|credit|\n",
      "|00000058|12-29-2011|4000002|194.86|        Water Sports|         Windsurfing| Oklahoma City|      Oklahoma|credit|\n",
      "|00000059|11-07-2011|4000001|021.43|       Winter Sports|        Snowboarding|  Philadelphia|  Pennsylvania|  cash|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1 = spark.read.csv('/E:/spark/spark-3.0.3-bin-hadoop2.7/BTGK/trans.txt')\n",
    "dftest1.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|         _c0|    _c1|        _c2|                _c3|  _c4|           _c5|  _c6|    _c7|  _c8|  _c9| _c10|       _c11|   _c12|                _c13|                _c14|         _c15|           _c16|               _c17|      _c18|         _c19|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        FALSE|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        FALSE|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        FALSE|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        FALSE|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        FALSE|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        FALSE|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        FALSE|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        FALSE|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        FALSE|            842|               1666|  28876493|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options(delimiter=',').csv(\"zipcodes.csv\")\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.options(inferSchema='True',delimiter=',').csv(\"zipcodes.csv\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: boolean (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options(inferSchema='True',delimiter=',').csv(\"zipcodesNoHeader.csv\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Xaxis: string (nullable = true)\n",
      " |-- Yaxis: string (nullable = true)\n",
      " |-- Zaxis: string (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: string (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: string (nullable = true)\n",
      " |-- TotalWages: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options(header = 'true', inferSchema = 'false', delimiter = ',').csv(\"zipcodes.csv\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType() \\\n",
    ".add (\"RecordNumber\", IntegerType(), True)\\\n",
    ".add (\"Zipcode\" ,IntegerType() ,True)\\\n",
    ".add (\"ZipCodeType\" ,StringType(), True)\\\n",
    ".add (\"City\" ,StringType(), True)\\\n",
    ".add (\"State\", StringType(), True)\\\n",
    ".add (\"LocationType\", StringType(), True)\\\n",
    ".add (\"Lat\" ,DoubleType(), True)\\\n",
    ".add (\"Long\" ,DoubleType(), True)\\\n",
    ".add (\"Xaxis\", IntegerType(), True)\\\n",
    ".add (\"Yaxis\", DoubleType(), True)\\\n",
    ".add (\"Zaxis\", DoubleType(), True)\\\n",
    ".add (\"WorldRegion\", StringType(), True)\\\n",
    ".add (\"Country\" ,StringType(), True)\\\n",
    ".add (\"LocationText\" ,StringType(), True)\\\n",
    ".add (\"Location\" ,StringType() ,True)\\\n",
    ".add (\"Decommisioned\" ,BooleanType(), True)\\\n",
    ".add (\"TaxReturnsFiled\", StringType(), True)\\\n",
    ".add (\"EstimatedPopulation\" ,IntegerType(), True)\\\n",
    ".add (\"TotalWages\", IntegerType(), True)\\\n",
    ".add (\"Notes\" ,StringType(), True)\n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(\"zipcodes.csv\")\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cust_ID: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_ID|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF = spark.read.options(delimiter=',').schema('trans_id INT, date STRING, cust_ID INT, amount DOUBLE, game STRING, equipment STRING, city STRING, state STRING, mode STRING').csv(\"trans.txt\")\n",
    "transDF.printSchema()\n",
    "transDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|         _c0|    _c1|        _c2|                _c3|  _c4|           _c5|  _c6|    _c7|  _c8|  _c9| _c10|       _c11|   _c12|                _c13|                _c14|         _c15|           _c16|               _c17|      _c18|         _c19|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        FALSE|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        FALSE|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        FALSE|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        FALSE|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        FALSE|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        FALSE|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        FALSE|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        FALSE|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        FALSE|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        FALSE|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        FALSE|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        FALSE|            842|               1666|  28876493|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/E:/spark/spark-3.0.3-bin-hadoop2.7/BTGK/newzipcodes already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-d6f2186680ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"newzipcodes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/E:/spark/spark-3.0.3-bin-hadoop2.7/BTGK/newzipcodes already exists.;"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\",True).csv(\"newzipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/E:/spark/spark-3.0.3-bin-hadoop2.7/BTGK/newzipcodes already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-8375c7083598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"newzipcodes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/E:/spark/spark-3.0.3-bin-hadoop2.7/BTGK/newzipcodes already exists.;"
     ]
    }
   ],
   "source": [
    "df2.write.options(header='True', delimiter=',').csv(\"newzipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1606.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 307, vannhinh-ng02.ea.corp.samsungelectronics.net, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\newzipcodes\\_temporary\\0\\_temporary\\attempt_202112171247472039298821884897827_0155_m_000000_307\\part-00000-3460f4cc-8ecf-4edf-be1b-43553d088430-c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\newzipcodes\\_temporary\\0\\_temporary\\attempt_202112171247472039298821884897827_0155_m_000000_307\\part-00000-3460f4cc-8ecf-4edf-be1b-43553d088430-c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-9ed8f365029d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"newzipcodes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1606.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 307, vannhinh-ng02.ea.corp.samsungelectronics.net, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\newzipcodes\\_temporary\\0\\_temporary\\attempt_202112171247472039298821884897827_0155_m_000000_307\\part-00000-3460f4cc-8ecf-4edf-be1b-43553d088430-c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\newzipcodes\\_temporary\\0\\_temporary\\attempt_202112171247472039298821884897827_0155_m_000000_307\\part-00000-3460f4cc-8ecf-4edf-be1b-43553d088430-c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df2.write.mode('overwrite').csv(\"newzipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='RecordNumber\\tZipcode\\tZipCodeType\\tCity\\tState\\tLocationType'),\n",
       " Row(value='1\\t704\\tSTANDARD\\tPARC PARQUE\\tPR\\tNOT ACCEPTABLE'),\n",
       " Row(value='2\\t704\\tSTANDARD\\tPASEO COSTA DEL SUR\\tPR\\tNOT ACCEPTABLE'),\n",
       " Row(value='10\\t709\\tSTANDARD\\tBDA SAN LUIS\\tPR\\tNOT ACCEPTABLE'),\n",
       " Row(value='61391\\t76166\\tUNIQUE\\tCINGULAR WIRELESS\\tTX\\tNOT ACCEPTABLE'),\n",
       " Row(value='61392\\t76177\\tSTANDARD\\tFORT WORTH\\tTX\\tPRIMARY'),\n",
       " Row(value='61393\\t76177\\tSTANDARD\\tFT WORTH\\tTX\\tACCEPTABLE'),\n",
       " Row(value='4\\t704\\tSTANDARD\\tURB EUGENE RICE\\tPR\\tNOT ACCEPTABLE'),\n",
       " Row(value='39827\\t85209\\tSTANDARD\\tMESA\\tAZ\\tPRIMARY'),\n",
       " Row(value='39828\\t85210\\tSTANDARD\\tMESA\\tAZ\\tPRIMARY'),\n",
       " Row(value='49345\\t32046\\tSTANDARD\\tHILLIARD\\tFL\\tPRIMARY'),\n",
       " Row(value='49346\\t34445\\tPO BOX\\tHOLDER\\tFL\\tPRIMARY'),\n",
       " Row(value='49347\\t32564\\tSTANDARD\\tHOLT\\tFL\\tPRIMARY'),\n",
       " Row(value='49348\\t34487\\tPO BOX\\tHOMOSASSA\\tFL\\tPRIMARY'),\n",
       " Row(value='10\\t708\\tSTANDARD\\tBDA SAN LUIS\\tPR\\tNOT ACCEPTABLE'),\n",
       " Row(value='3\\t704\\tSTANDARD\\tSECT LANAUSSE\\tPR\\tNOT ACCEPTABLE'),\n",
       " Row(value='54354\\t36275\\tPO BOX\\tSPRING GARDEN\\tAL\\tPRIMARY'),\n",
       " Row(value='54355\\t35146\\tSTANDARD\\tSPRINGVILLE\\tAL\\tPRIMARY'),\n",
       " Row(value='54356\\t35585\\tSTANDARD\\tSPRUCE PINE\\tAL\\tPRIMARY'),\n",
       " Row(value='76511\\t27007\\tSTANDARD\\tASH HILL\\tNC\\tNOT ACCEPTABLE'),\n",
       " Row(value='76512\\t27203\\tSTANDARD\\tASHEBORO\\tNC\\tPRIMARY'),\n",
       " Row(value='76513\\t27204\\tPO BOX\\tASHEBORO\\tNC\\tPRIMARY')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(\"zipcodes.txt\")\n",
    "df.printSchema()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header='True',inferSchema='True', delimiter='\\t').csv(\"zipcodes.txt\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "transDF = spark.read.options(delimiter=',').schema('trans_id INT, date STRING, cust_id INT, amount DOUBLE, game STRING, equipment STRING, city STRING, state STRING, mode STRING').csv(\"trans.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cust_id|amount|\n",
      "+-------+------+\n",
      "|4000001| 40.33|\n",
      "|4000002|198.44|\n",
      "|4000002|  5.58|\n",
      "|4000003|198.19|\n",
      "|4000002| 98.81|\n",
      "|4000004|193.63|\n",
      "|4000005| 27.89|\n",
      "|4000006| 96.01|\n",
      "|4000006| 10.44|\n",
      "|4000006|152.46|\n",
      "|4000007|180.28|\n",
      "|4000009|121.39|\n",
      "|4000009| 41.52|\n",
      "|4000010| 107.8|\n",
      "|4000010| 36.81|\n",
      "|4000001|137.64|\n",
      "|4000010| 35.56|\n",
      "|4000008| 75.55|\n",
      "|4000008| 88.65|\n",
      "|4000008| 51.81|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.select('cust_id','amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cust_id|amount|\n",
      "+-------+------+\n",
      "|4000001| 40.33|\n",
      "|4000002|198.44|\n",
      "|4000002|  5.58|\n",
      "|4000003|198.19|\n",
      "|4000002| 98.81|\n",
      "|4000004|193.63|\n",
      "|4000005| 27.89|\n",
      "|4000006| 96.01|\n",
      "|4000006| 10.44|\n",
      "|4000006|152.46|\n",
      "|4000007|180.28|\n",
      "|4000009|121.39|\n",
      "|4000009| 41.52|\n",
      "|4000010| 107.8|\n",
      "|4000010| 36.81|\n",
      "|4000001|137.64|\n",
      "|4000010| 35.56|\n",
      "|4000008| 75.55|\n",
      "|4000008| 88.65|\n",
      "|4000008| 51.81|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.select(transDF.cust_id,transDF.amount).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cust_id|amount|\n",
      "+-------+------+\n",
      "|4000001| 40.33|\n",
      "|4000002|198.44|\n",
      "|4000002|  5.58|\n",
      "|4000003|198.19|\n",
      "|4000002| 98.81|\n",
      "|4000004|193.63|\n",
      "|4000005| 27.89|\n",
      "|4000006| 96.01|\n",
      "|4000006| 10.44|\n",
      "|4000006|152.46|\n",
      "|4000007|180.28|\n",
      "|4000009|121.39|\n",
      "|4000009| 41.52|\n",
      "|4000010| 107.8|\n",
      "|4000010| 36.81|\n",
      "|4000001|137.64|\n",
      "|4000010| 35.56|\n",
      "|4000008| 75.55|\n",
      "|4000008| 88.65|\n",
      "|4000008| 51.81|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.select(transDF['cust_id'],transDF['amount']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cust_id|amount|\n",
      "+-------+------+\n",
      "|4000001| 40.33|\n",
      "|4000002|198.44|\n",
      "|4000002|  5.58|\n",
      "|4000003|198.19|\n",
      "|4000002| 98.81|\n",
      "|4000004|193.63|\n",
      "|4000005| 27.89|\n",
      "|4000006| 96.01|\n",
      "|4000006| 10.44|\n",
      "|4000006|152.46|\n",
      "|4000007|180.28|\n",
      "|4000009|121.39|\n",
      "|4000009| 41.52|\n",
      "|4000010| 107.8|\n",
      "|4000010| 36.81|\n",
      "|4000001|137.64|\n",
      "|4000010| 35.56|\n",
      "|4000008| 75.55|\n",
      "|4000008| 88.65|\n",
      "|4000008| 51.81|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twocolumns=['cust_id','amount']\n",
    "transDF.select(twocolumns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.select([col for col in transDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "transDF.withColumn('trans_id',col('trans_id').cast('String')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.withColumn('trans_id',col('trans_id').cast('integer')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 80.66|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|396.88|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002| 11.16|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|396.38|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002|197.62|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|387.26|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 55.78|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006|192.02|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 20.88|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|304.92|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|360.56|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|242.78|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 83.04|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 215.6|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 73.62|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|275.28|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 71.12|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 151.1|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 177.3|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008|103.62|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "transDF.withColumn('amount',col('amount')*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+----------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|new amount|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+----------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|     80.66|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|    396.88|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|     11.16|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|    396.38|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|    197.62|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|    387.26|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|     55.78|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|    192.02|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|     20.88|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|    304.92|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|    360.56|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|    242.78|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|     83.04|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|     215.6|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|     73.62|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|    275.28|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|     71.12|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|     151.1|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|     177.3|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|    103.62|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "transDF.withColumn('new amount',col('amount')*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+-------+\n",
      "|trans_id|      date|cust_id|amount|                game|           equipment|          city|         state|  mode|Country|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+-------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|    USA|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|    USA|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|    USA|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|    USA|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|    USA|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|    USA|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|    USA|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|    USA|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|    USA|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|    USA|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|    USA|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|    USA|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|    USA|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|    USA|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|    USA|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|    USA|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|    USA|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|    USA|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|    USA|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|    USA|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "transDF.withColumn(\"Country\", lit(\"USA\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_id|  cost|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "transDF.withColumnRenamed('amount','cost').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |\n",
      "|[Anna, Rose, ]        |[Spark, Java, C++]|NY   |F     |\n",
      "|[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |\n",
      "|[Maria, Anne, Jones]  |[CSharp, VB]      |NY   |M     |\n",
      "|[Jen, Mary, Brown]    |[CSharp, VB]      |NY   |M     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "data = [\n",
    "((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    "]\n",
    "schema = StructType([\n",
    "StructField('name', StructType([\n",
    "StructField('firstname', StringType(), True),\n",
    "StructField('middlename', StringType(), True),\n",
    "StructField('lastname', StringType(), True)\n",
    "])),\n",
    "StructField('languages', ArrayType(StringType()), True),\n",
    "StructField('state', StringType(), True),\n",
    "StructField('gender', StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |\n",
      "|[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['state'] == \"OH\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|[Anna, Rose, ]      |[Spark, Java, C++]|NY   |F     |\n",
      "|[Maria, Anne, Jones]|[CSharp, VB]      |NY   |M     |\n",
      "|[Jen, Mary, Brown]  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state != \"OH\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|[Anna, Rose, ]      |[Spark, Java, C++]|NY   |F     |\n",
      "|[Maria, Anne, Jones]|[CSharp, VB]      |NY   |M     |\n",
      "|[Jen, Mary, Brown]  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~(df.state == \"OH\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |\n",
      "|[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"state\") == \"OH\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"gender == 'M'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"gender != 'M'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"gender <> 'M'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.state == 'OH') & (df.gender =='M')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li=[\"OH\",\"CA\",\"DE\"]\n",
    "df.filter(df.state.isin(li)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~df.state.isin(li)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.isin(li) == False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.startswith(\"N\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.endswith(\"H\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.contains(\"Y\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),(4,\"Rames Rose\"),(5,\"Rames rose\")]\n",
    "df2 = spark.createDataFrame(data = data2, schema=[\"id\",\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      name|\n",
      "+---+----------+\n",
      "|  5|Rames rose|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(df2.name.like(\"%rose%\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|        name|\n",
      "+---+------------+\n",
      "|  2|Michael Rose|\n",
      "|  4|  Rames Rose|\n",
      "|  5|  Rames rose|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n",
      "|            name|         languages|state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|[James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|  [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "+----------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df.languages,\"Java\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-----+------+\n",
      "|name                  |languages   |state|gender|\n",
      "+----------------------+------------+-----+------+\n",
      "|[Julia, , Williams]   |[CSharp, VB]|OH   |F     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]|OH   |M     |\n",
      "+----------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.lastname ==\"Williams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-----+------+\n",
      "|name                  |languages   |state|gender|\n",
      "+----------------------+------------+-----+------+\n",
      "|[Julia, , Williams]   |[CSharp, VB]|OH   |F     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]|OH   |M     |\n",
      "+----------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.name.lastname ==\"Williams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transDF.select('cust_id','game').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transDF.select('cust_id','game').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of customer ID & game : 43\n",
      "+--------+----------+-------+------+----------------------+-------------------+-------------+--------------+------+\n",
      "|trans_id|date      |cust_id|amount|game                  |equipment          |city         |state         |mode  |\n",
      "+--------+----------+-------+------+----------------------+-------------------+-------------+--------------+------+\n",
      "|13      |03-13-2011|4000010|107.8 |Team Sports           |Field Hockey       |Honolulu     |Hawaii        |credit|\n",
      "|48      |09-27-2011|4000007|157.94|Exercise & Fitness    |Exercise Bands     |Philadelphia |Pennsylvania  |credit|\n",
      "|20      |06-29-2011|4000005|41.55 |Exercise & Fitness    |Weightlifting Belts|New Orleans  |Louisiana     |credit|\n",
      "|33      |06-15-2011|4000008|154.15|Outdoor Recreation    |Lawn Games         |Nashville    |Tennessee     |credit|\n",
      "|49      |07-12-2011|4000010|144.59|Jumping               |Jumping Stilts     |Cambridge    |Massachusetts |credit|\n",
      "|46      |05-27-2011|4000001|52.29 |Gymnastics            |Vaulting Horses    |Cleveland    |Ohio          |credit|\n",
      "|55      |12-16-2011|4000006|106.11|Water Sports          |Swimming           |New York     |New York      |credit|\n",
      "|3       |06-05-2011|4000003|198.19|Gymnastics            |Gymnastics Rings   |Milwaukee    |Wisconsin     |credit|\n",
      "|6       |10-28-2011|4000005|27.89 |Puzzles               |Jigsaw Puzzles     |Charleston   |South Carolina|credit|\n",
      "|12      |02-08-2011|4000009|41.52 |Indoor Games          |Bowling            |San Francisco|California    |credit|\n",
      "|10      |05-29-2011|4000007|180.28|Outdoor Recreation    |Archery            |Reno         |Nevada        |credit|\n",
      "|47      |10-23-2011|4000008|100.1 |Outdoor Play Equipment|Swing Sets         |Everett      |Washington    |credit|\n",
      "|24      |06-10-2011|4000003|151.2 |Water Sports          |Surfing            |Plano        |Texas         |credit|\n",
      "|52      |02-04-2011|4000005|44.82 |Outdoor Play Equipment|Lawn Water Slides  |Hampton      |Virginia      |cash  |\n",
      "|15      |10-20-2011|4000001|137.64|Combat Sports         |Fencing            |Honolulu     |Hawaii        |credit|\n",
      "|43      |04-22-2011|4000004|32.34 |Water Sports          |Water Polo         |Las Vegas    |Nevada        |cash  |\n",
      "|16      |05-28-2011|4000010|35.56 |Exercise & Fitness    |Free Weight Bars   |Columbia     |South Carolina|credit|\n",
      "|14      |02-25-2011|4000010|36.81 |Gymnastics            |Vaulting Horses    |Los Angeles  |California    |credit|\n",
      "|22      |10-10-2011|4000009|19.64 |Water Sports          |Kitesurfing        |Saint Paul   |Minnesota     |credit|\n",
      "|7       |07-14-2011|4000006|96.01 |Outdoor Play Equipment|Sandboxes          |Columbus     |Ohio          |credit|\n",
      "+--------+----------+-------+------+----------------------+-------------------+-------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropDisDF = transDF.dropDuplicates(['cust_id','game'])\n",
    "print(\"Distinct count of customer ID & game : \" + str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+------------------+--------------------------+-----------+----------+------+\n",
      "|trans_id|date      |cust_ID|amount|game              |equipment                 |city       |state     |mode  |\n",
      "+--------+----------+-------+------+------------------+--------------------------+-----------+----------+------+\n",
      "|0       |06-26-2011|4000001|40.33 |Exercise & Fitness|Cardio Machine Accessories|Clarksville|Tennessee |credit|\n",
      "|1       |05-26-2011|4000002|198.44|Exercise & Fitness|Weightlifting Gloves      |Long Beach |California|credit|\n",
      "+--------+----------+-------+------+------------------+--------------------------+-----------+----------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF = spark.read.options(delimiter=',')\\\n",
    ".schema('trans_id INT, date STRING, cust_ID INT, amount DOUBLE, game STRING, equipment STRING, city STRING, state STRING, mode STRING')\\\n",
    ".csv(\"trans.txt\")\n",
    "\n",
    "transDF.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+----------------------+------------------------------+-------------+-------------+------+\n",
      "|trans_id|date      |cust_ID|amount|game                  |equipment                     |city         |state        |mode  |\n",
      "+--------+----------+-------+------+----------------------+------------------------------+-------------+-------------+------+\n",
      "|1       |05-26-2011|4000002|198.44|Exercise & Fitness    |Weightlifting Gloves          |Long Beach   |California   |credit|\n",
      "|3       |06-05-2011|4000003|198.19|Gymnastics            |Gymnastics Rings              |Milwaukee    |Wisconsin    |credit|\n",
      "|58      |12-29-2011|4000002|194.86|Water Sports          |Windsurfing                   |Oklahoma City|Oklahoma     |credit|\n",
      "|5       |02-14-2011|4000004|193.63|Outdoor Recreation    |Camping & Backpacking & Hiking|Chicago      |Illinois     |credit|\n",
      "|35      |04-12-2011|4000008|185.26|Games                 |Board Games                   |Centennial   |Colorado     |credit|\n",
      "|10      |05-29-2011|4000007|180.28|Outdoor Recreation    |Archery                       |Reno         |Nevada       |credit|\n",
      "|57      |12-20-2011|4000003|178.2 |Outdoor Recreation    |Skating                       |San Jose     |California   |credit|\n",
      "|56      |06-21-2011|4000002|176.63|Outdoor Recreation    |Geocaching                    |Boston       |Massachusetts|credit|\n",
      "|39      |03-12-2011|4000006|174.36|Outdoor Play Equipment|Swing Sets                    |Pittsburgh   |Pennsylvania |credit|\n",
      "|40      |11-07-2011|4000005|165.1 |Team Sports           |Cheerleading                  |Reno         |Nevada       |credit|\n",
      "+--------+----------+-------+------+----------------------+------------------------------+-------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.sort('amount',ascending = False).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_ID|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|       vietnam|  cash|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|       vietnam|  cash|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |       vietnam|  cash|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "transDF.withColumn('state',when(transDF.cust_ID=='4000002','vietnam').otherwise(transDF.state)) \\\n",
    "    .withColumn('mode',when(transDF.cust_ID=='4000002','cash').otherwise(transDF.mode)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cust_ID|count|\n",
      "+-------+-----+\n",
      "|4000001|    8|\n",
      "|4000002|    6|\n",
      "|4000003|    3|\n",
      "|4000004|    5|\n",
      "|4000005|    5|\n",
      "|4000006|    5|\n",
      "|4000007|    6|\n",
      "|4000008|   10|\n",
      "|4000009|    6|\n",
      "|4000010|    6|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').count().sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+-----------+\n",
      "|cust_id|min(trans_id)|min(cust_ID)|min(amount)|\n",
      "+-------+-------------+------------+-----------+\n",
      "|4000009|           11|     4000009|      19.64|\n",
      "|4000001|            0|     4000001|      21.43|\n",
      "|4000006|            7|     4000006|      10.44|\n",
      "|4000005|            6|     4000005|      27.89|\n",
      "|4000008|           17|     4000008|       5.03|\n",
      "+-------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_id').min().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|cust_id|min(amount)|\n",
      "+-------+-----------+\n",
      "|4000009|      19.64|\n",
      "|4000001|      21.43|\n",
      "|4000006|      10.44|\n",
      "|4000005|      27.89|\n",
      "|4000008|       5.03|\n",
      "+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_id').min('amount').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|cust_ID|min_transaction_amount|\n",
      "+-------+----------------------+\n",
      "|4000009|                 19.64|\n",
      "|4000001|                 21.43|\n",
      "|4000006|                 10.44|\n",
      "|4000005|                 27.89|\n",
      "|4000008|                  5.03|\n",
      "+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "transDF.groupBy('cust_ID').agg(f.min('amount').alias('min_transaction_amount')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|cust_id|min_trans|\n",
      "+-------+---------+\n",
      "|4000009|    19.64|\n",
      "|4000001|    21.43|\n",
      "|4000006|    10.44|\n",
      "|4000005|    27.89|\n",
      "|4000008|     5.03|\n",
      "|4000004|    28.11|\n",
      "|4000003|    151.2|\n",
      "|4000010|    35.56|\n",
      "|4000007|     20.2|\n",
      "|4000002|     5.58|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_id').agg(f.min('amount').alias('min_trans')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|cust_id|min(amount)|\n",
      "+-------+-----------+\n",
      "|4000001|      21.43|\n",
      "|4000002|       5.58|\n",
      "|4000003|      151.2|\n",
      "|4000004|      28.11|\n",
      "|4000005|      27.89|\n",
      "|4000006|      10.44|\n",
      "|4000007|       20.2|\n",
      "|4000008|       5.03|\n",
      "|4000009|      19.64|\n",
      "|4000010|      35.56|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_id').agg(f.min('amount')).sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|cust_id|       sum(amount)|\n",
      "+-------+------------------+\n",
      "|4000009|            457.83|\n",
      "|4000001|            651.05|\n",
      "|4000006|            539.38|\n",
      "|4000005|            325.15|\n",
      "|4000008|            859.42|\n",
      "|4000004|            337.06|\n",
      "|4000003| 527.5899999999999|\n",
      "|4000010|447.09000000000003|\n",
      "|4000007| 699.5500000000001|\n",
      "|4000002|            706.97|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_id').sum('amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|cust_id|      total_amount|\n",
      "+-------+------------------+\n",
      "|4000009|            457.83|\n",
      "|4000001|            651.05|\n",
      "|4000006|            539.38|\n",
      "|4000005|            325.15|\n",
      "|4000008|            859.42|\n",
      "|4000004|            337.06|\n",
      "|4000003| 527.5899999999999|\n",
      "|4000010|447.09000000000003|\n",
      "|4000007| 699.5500000000001|\n",
      "|4000002|            706.97|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_id').agg(f.sum('amount').alias('total_amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|cust_ID|         first(game)|\n",
      "+-------+--------------------+\n",
      "|4000009|Outdoor Play Equi...|\n",
      "|4000001|  Exercise & Fitness|\n",
      "|4000006|Outdoor Play Equi...|\n",
      "|4000005|             Puzzles|\n",
      "|4000008|        Water Sports|\n",
      "|4000004|  Outdoor Recreation|\n",
      "|4000003|          Gymnastics|\n",
      "|4000010|         Team Sports|\n",
      "|4000007|  Outdoor Recreation|\n",
      "|4000002|  Exercise & Fitness|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').agg(f.first('game')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+------------------+--------------------+-------------+-------------+------+\n",
      "|trans_id|      date|cust_ID|amount|              game|           equipment|         city|        state|  mode|\n",
      "+--------+----------+-------+------+------------------+--------------------+-------------+-------------+------+\n",
      "|       1|05-26-2011|4000002|198.44|Exercise & Fitness|Weightlifting Gloves|   Long Beach|   California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|Exercise & Fitness|Weightlifting Mac...|      Anaheim|   California|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|       Team Sports|        Field Hockey|  Nashville  |    Tennessee|credit|\n",
      "|      51|02-17-2011|4000002| 32.65|      Water Sports|        Life Jackets|     Columbus|      Georgia|  cash|\n",
      "|      56|06-21-2011|4000002|176.63|Outdoor Recreation|          Geocaching|       Boston|Massachusetts|credit|\n",
      "|      58|12-29-2011|4000002|194.86|      Water Sports|         Windsurfing|Oklahoma City|     Oklahoma|credit|\n",
      "+--------+----------+-------+------+------------------+--------------------+-------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.filter(transDF.cust_ID == 4000002).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+\n",
      "|cust_id|count(cust_id)|       sum(amount)|\n",
      "+-------+--------------+------------------+\n",
      "|4000010|             6|447.09000000000003|\n",
      "|4000009|             6|            457.83|\n",
      "|4000008|            10|            859.42|\n",
      "|4000007|             6| 699.5500000000001|\n",
      "|4000006|             5|            539.38|\n",
      "|4000005|             5|            325.15|\n",
      "|4000004|             5|            337.06|\n",
      "|4000003|             3| 527.5899999999999|\n",
      "|4000002|             6|            706.97|\n",
      "|4000001|             8|            651.05|\n",
      "+-------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = transDF.groupBy('cust_id').agg(f.count('cust_id'))\n",
    "t2 = transDF.groupBy('cust_id').agg(f.sum('amount'))\n",
    "t3 = t1.join(t2,['cust_id'],\"inner\")\n",
    "t3.sort('cust_id', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|cust_ID|count(game)|\n",
      "+-------+-----------+\n",
      "|4000009|          5|\n",
      "|4000001|          6|\n",
      "|4000006|          4|\n",
      "|4000005|          5|\n",
      "|4000008|          5|\n",
      "|4000004|          3|\n",
      "|4000003|          3|\n",
      "|4000010|          5|\n",
      "|4000007|          3|\n",
      "|4000002|          4|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').agg(f.countDistinct('game')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cust_ID|collect_list(game)                                                                                                                        |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|4000009|[Outdoor Play Equipment, Indoor Games, Water Sports, Gymnastics, Indoor Games, Combat Sports]                                             |\n",
      "|4000001|[Exercise & Fitness, Combat Sports, Outdoor Recreation, Water Sports, Water Sports, Exercise & Fitness, Gymnastics, Winter Sports]        |\n",
      "|4000006|[Outdoor Play Equipment, Winter Sports, Jumping, Outdoor Play Equipment, Water Sports]                                                    |\n",
      "|4000005|[Puzzles, Exercise & Fitness, Air Sports, Team Sports, Outdoor Play Equipment]                                                            |\n",
      "|4000008|[Water Sports, Team Sports, Water Sports, Team Sports, Games, Team Sports, Outdoor Recreation, Team Sports, Games, Outdoor Play Equipment]|\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').agg(f.collect_list('game')).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------+\n",
      "|cust_ID|collect_set(game)                                                                               |\n",
      "+-------+------------------------------------------------------------------------------------------------+\n",
      "|4000009|[Combat Sports, Water Sports, Indoor Games, Gymnastics, Outdoor Play Equipment]                 |\n",
      "|4000001|[Combat Sports, Water Sports, Outdoor Recreation, Gymnastics, Winter Sports, Exercise & Fitness]|\n",
      "|4000006|[Water Sports, Jumping, Winter Sports, Outdoor Play Equipment]                                  |\n",
      "|4000005|[Puzzles, Team Sports, Air Sports, Exercise & Fitness, Outdoor Play Equipment]                  |\n",
      "|4000008|[Team Sports, Water Sports, Outdoor Recreation, Games, Outdoor Play Equipment]                  |\n",
      "+-------+------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').agg(f.collect_set('game')).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------+\n",
      "|cust_ID|collect_set(game)                                             |\n",
      "+-------+--------------------------------------------------------------+\n",
      "|4000006|[Water Sports, Jumping, Winter Sports, Outdoor Play Equipment]|\n",
      "|4000010|[Team Sports, Jumping, Gymnastics, Games, Exercise & Fitness] |\n",
      "+-------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').agg(f.collect_set('game')).filter(f.array_contains(f.col('collect_set(game)'),'Jumping')).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+\n",
      "|cust_ID|collect_set(game)                                                                               |game_list_string                                                                         |\n",
      "+-------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+\n",
      "|4000009|[Combat Sports, Water Sports, Indoor Games, Gymnastics, Outdoor Play Equipment]                 |Combat Sports,Water Sports,Indoor Games,Gymnastics,Outdoor Play Equipment                |\n",
      "|4000001|[Combat Sports, Water Sports, Outdoor Recreation, Gymnastics, Winter Sports, Exercise & Fitness]|Combat Sports,Water Sports,Outdoor Recreation,Gymnastics,Winter Sports,Exercise & Fitness|\n",
      "|4000006|[Water Sports, Jumping, Winter Sports, Outdoor Play Equipment]                                  |Water Sports,Jumping,Winter Sports,Outdoor Play Equipment                                |\n",
      "|4000005|[Puzzles, Team Sports, Air Sports, Exercise & Fitness, Outdoor Play Equipment]                  |Puzzles,Team Sports,Air Sports,Exercise & Fitness,Outdoor Play Equipment                 |\n",
      "|4000008|[Team Sports, Water Sports, Outdoor Recreation, Games, Outdoor Play Equipment]                  |Team Sports,Water Sports,Outdoor Recreation,Games,Outdoor Play Equipment                 |\n",
      "+-------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.groupBy('cust_ID').agg(f.collect_set('game')).withColumn('game_list_string',f.concat_ws(',',f.col('collect_set(game)'))).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "transGameStringDF =  transDF.groupBy('cust_id').agg(f.collect_set('game')).withColumn('game_string',f.concat_ws(',',f.col('collect_set(game)'))).select('cust_ID','game_string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------+\n",
      "|cust_ID|game_string                                                                              |\n",
      "+-------+-----------------------------------------------------------------------------------------+\n",
      "|4000009|Combat Sports,Water Sports,Indoor Games,Gymnastics,Outdoor Play Equipment                |\n",
      "|4000001|Combat Sports,Water Sports,Outdoor Recreation,Gymnastics,Winter Sports,Exercise & Fitness|\n",
      "|4000006|Water Sports,Jumping,Winter Sports,Outdoor Play Equipment                                |\n",
      "|4000005|Puzzles,Team Sports,Air Sports,Exercise & Fitness,Outdoor Play Equipment                 |\n",
      "|4000008|Team Sports,Water Sports,Outdoor Recreation,Games,Outdoor Play Equipment                 |\n",
      "+-------+-----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transGameStringDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n",
      "|cust_ID|game_string                                                                              |game_array                                                                                      |\n",
      "+-------+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n",
      "|4000009|Combat Sports,Water Sports,Indoor Games,Gymnastics,Outdoor Play Equipment                |[Combat Sports, Water Sports, Indoor Games, Gymnastics, Outdoor Play Equipment]                 |\n",
      "|4000001|Combat Sports,Water Sports,Outdoor Recreation,Gymnastics,Winter Sports,Exercise & Fitness|[Combat Sports, Water Sports, Outdoor Recreation, Gymnastics, Winter Sports, Exercise & Fitness]|\n",
      "|4000006|Water Sports,Jumping,Winter Sports,Outdoor Play Equipment                                |[Water Sports, Jumping, Winter Sports, Outdoor Play Equipment]                                  |\n",
      "|4000005|Puzzles,Team Sports,Air Sports,Exercise & Fitness,Outdoor Play Equipment                 |[Puzzles, Team Sports, Air Sports, Exercise & Fitness, Outdoor Play Equipment]                  |\n",
      "|4000008|Team Sports,Water Sports,Outdoor Recreation,Games,Outdoor Play Equipment                 |[Team Sports, Water Sports, Outdoor Recreation, Games, Outdoor Play Equipment]                  |\n",
      "+-------+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transGameStringDF.withColumn('game_array',f.split('game_string',',')).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------+\n",
      "|cust_ID|         game_string|          game_array|num_game|\n",
      "+-------+--------------------+--------------------+--------+\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|       5|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|       6|\n",
      "|4000006|Water Sports,Jump...|[Water Sports, Ju...|       4|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|       5|\n",
      "|4000008|Team Sports,Water...|[Team Sports, Wat...|       5|\n",
      "+-------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transGameStringDF.withColumn('game_array', f.split('game_string',',')).withColumn('num_game',f.size('game_array')).show(5,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------+--------------------+\n",
      "|cust_ID|         game_string|          game_array|   first_game|           last_game|\n",
      "+-------+--------------------+--------------------+-------------+--------------------+\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|Combat Sports|Outdoor Play Equi...|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|Combat Sports|  Exercise & Fitness|\n",
      "|4000006|Water Sports,Jump...|[Water Sports, Ju...| Water Sports|Outdoor Play Equi...|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|      Puzzles|Outdoor Play Equi...|\n",
      "|4000008|Team Sports,Water...|[Team Sports, Wat...|  Team Sports|Outdoor Play Equi...|\n",
      "+-------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transGameStringDF.withColumn('game_array',f.split('game_string',',')) \\\n",
    ".withColumn('first_game',f.element_at('game_array',1)) \\\n",
    ".withColumn('last_game',f.element_at('game_array',-1)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|cust_ID|         game_string|          game_array|         single_game|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|       Combat Sports|\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|        Water Sports|\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|        Indoor Games|\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|          Gymnastics|\n",
      "|4000009|Combat Sports,Wat...|[Combat Sports, W...|Outdoor Play Equi...|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|       Combat Sports|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|        Water Sports|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|  Outdoor Recreation|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|          Gymnastics|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|       Winter Sports|\n",
      "|4000001|Combat Sports,Wat...|[Combat Sports, W...|  Exercise & Fitness|\n",
      "|4000006|Water Sports,Jump...|[Water Sports, Ju...|        Water Sports|\n",
      "|4000006|Water Sports,Jump...|[Water Sports, Ju...|             Jumping|\n",
      "|4000006|Water Sports,Jump...|[Water Sports, Ju...|       Winter Sports|\n",
      "|4000006|Water Sports,Jump...|[Water Sports, Ju...|Outdoor Play Equi...|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|             Puzzles|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|         Team Sports|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|          Air Sports|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|  Exercise & Fitness|\n",
      "|4000005|Puzzles,Team Spor...|[Puzzles, Team Sp...|Outdoor Play Equi...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transGameStringDF.withColumn('game_array',f.split('game_string',',')).withColumn('single_game',f.explode('game_array')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|trans_id|      date|cust_ID|amount|                game|           equipment|          city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+---------+\n",
      "|trans_id|      date|cust_ID|amount|                game|           equipment|          city|         state|  mode|day_month|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+---------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|   Clarksville|     Tennessee|credit|    06-26|\n",
      "|       1|05-26-2011|4000002|198.44|  Exercise & Fitness|Weightlifting Gloves|    Long Beach|    California|credit|    05-26|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|       Anaheim|    California|credit|    06-01|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|     Milwaukee|     Wisconsin|credit|    06-05|\n",
      "|       4|12-17-2011|4000002| 98.81|         Team Sports|        Field Hockey|   Nashville  |     Tennessee|credit|    12-17|\n",
      "|       5|02-14-2011|4000004|193.63|  Outdoor Recreation|Camping & Backpac...|       Chicago|      Illinois|credit|    02-14|\n",
      "|       6|10-28-2011|4000005| 27.89|             Puzzles|      Jigsaw Puzzles|    Charleston|South Carolina|credit|    10-28|\n",
      "|       7|07-14-2011|4000006| 96.01|Outdoor Play Equi...|           Sandboxes|      Columbus|          Ohio|credit|    07-14|\n",
      "|       8|01-17-2011|4000006| 10.44|       Winter Sports|        Snowmobiling|    Des Moines|          Iowa|credit|    01-17|\n",
      "|       9|05-17-2011|4000006|152.46|             Jumping|      Bungee Jumping|St. Petersburg|       Florida|credit|    05-17|\n",
      "|      10|05-29-2011|4000007|180.28|  Outdoor Recreation|             Archery|          Reno|        Nevada|credit|    05-29|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|      Columbus|          Ohio|credit|    06-18|\n",
      "|      12|02-08-2011|4000009| 41.52|        Indoor Games|             Bowling| San Francisco|    California|credit|    02-08|\n",
      "|      13|03-13-2011|4000010| 107.8|         Team Sports|        Field Hockey|    Honolulu  |        Hawaii|credit|    03-13|\n",
      "|      14|02-25-2011|4000010| 36.81|          Gymnastics|     Vaulting Horses|   Los Angeles|    California|credit|    02-25|\n",
      "|      15|10-20-2011|4000001|137.64|       Combat Sports|             Fencing|    Honolulu  |        Hawaii|credit|    10-20|\n",
      "|      16|05-28-2011|4000010| 35.56|  Exercise & Fitness|    Free Weight Bars|      Columbia|South Carolina|credit|    05-28|\n",
      "|      17|10-18-2011|4000008| 75.55|        Water Sports|Scuba Diving & Sn...|         Omaha|      Nebraska|credit|    10-18|\n",
      "|      18|11-18-2011|4000008| 88.65|         Team Sports|            Baseball|Salt Lake City|          Utah|credit|    11-18|\n",
      "|      19|08-28-2011|4000008| 51.81|        Water Sports|        Life Jackets|        Newark|    New Jersey|credit|    08-28|\n",
      "+--------+----------+-------+------+--------------------+--------------------+--------------+--------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.show()\n",
    "transDF.withColumn('day_month',f.substring('date',1,5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|06-26-2011|\n",
      "|05-26-2011|\n",
      "|06-01-2011|\n",
      "|06-05-2011|\n",
      "|12-17-2011|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+\n",
      "|day_month_year|\n",
      "+--------------+\n",
      "|    06-26-2011|\n",
      "|    05-26-2011|\n",
      "|    06-01-2011|\n",
      "|    06-05-2011|\n",
      "|    12-17-2011|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.select(f.col('date')).show(5)\n",
    "transDF.select(f.col('date').alias('day_month_year')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+------------------+--------------------+-----------+----------+------+--------------+\n",
      "|trans_id|      date|cust_ID|amount|              game|           equipment|       city|     state|  mode|month_in_05_06|\n",
      "+--------+----------+-------+------+------------------+--------------------+-----------+----------+------+--------------+\n",
      "|       0|06-26-2011|4000001| 40.33|Exercise & Fitness|Cardio Machine Ac...|Clarksville| Tennessee|credit|         false|\n",
      "|       1|05-26-2011|4000002|198.44|Exercise & Fitness|Weightlifting Gloves| Long Beach|California|credit|          true|\n",
      "|       2|06-01-2011|4000002|  5.58|Exercise & Fitness|Weightlifting Mac...|    Anaheim|California|credit|         false|\n",
      "|       3|06-05-2011|4000003|198.19|        Gymnastics|    Gymnastics Rings|  Milwaukee| Wisconsin|credit|         false|\n",
      "|       4|12-17-2011|4000002| 98.81|       Team Sports|        Field Hockey|Nashville  | Tennessee|credit|          true|\n",
      "+--------+----------+-------+------+------------------+--------------------+-----------+----------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.withColumn('month_in_05_06',f.substring('date',1,2).isin(['05','12'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+--------------------+--------------------+-----------+--------------+------+\n",
      "|trans_id|      date|cust_ID|amount|                game|           equipment|       city|         state|  mode|\n",
      "+--------+----------+-------+------+--------------------+--------------------+-----------+--------------+------+\n",
      "|       0|06-26-2011|4000001| 40.33|  Exercise & Fitness|Cardio Machine Ac...|Clarksville|     Tennessee|credit|\n",
      "|       2|06-01-2011|4000002|  5.58|  Exercise & Fitness|Weightlifting Mac...|    Anaheim|    California|credit|\n",
      "|       3|06-05-2011|4000003|198.19|          Gymnastics|    Gymnastics Rings|  Milwaukee|     Wisconsin|credit|\n",
      "|      11|06-18-2011|4000009|121.39|Outdoor Play Equi...|          Swing Sets|   Columbus|          Ohio|credit|\n",
      "|      20|06-29-2011|4000005| 41.55|  Exercise & Fitness| Weightlifting Belts|New Orleans|     Louisiana|credit|\n",
      "|      24|06-10-2011|4000003| 151.2|        Water Sports|             Surfing|      Plano|         Texas|credit|\n",
      "|      29|06-03-2011|4000001| 126.9|  Outdoor Recreation|             Hunting|    Phoenix|       Arizona|credit|\n",
      "|      33|06-15-2011|4000008|154.15|  Outdoor Recreation|          Lawn Games|Nashville  |     Tennessee|credit|\n",
      "|      53|06-12-2011|4000004| 44.46|        Water Sports|Scuba Diving & Sn...| Charleston|South Carolina|  cash|\n",
      "|      56|06-21-2011|4000002|176.63|  Outdoor Recreation|          Geocaching|     Boston| Massachusetts|credit|\n",
      "+--------+----------+-------+------+--------------------+--------------------+-----------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.filter(f.substring('date',0,2).isin(['06'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cust_ID: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cust_ID: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transDF.withColumn('month',f.substring('date',0,2)).printSchema()\n",
    "transDF.withColumn('month',f.substring('date',0,2).cast('INT')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-4cbbffdfdb64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_inner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Roll_No'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_inner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df_inner = df1.join(df2, on=['Roll_No'], how='inner')\n",
    "df_inner.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark =SparkSession.builder.appName(\"PySparktutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (spark.read.csv(path= \"ratings_small.csv\",sep=\",\",header=True,quote='\"\"',schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",)\\\n",
    "    .withColumnRenamed(\"timestamp\",\"timestamp_unix\")\\\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\"))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2867.showString.\n: java.lang.RuntimeException: quote cannot be more than one character\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.getChar(CSVOptions.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:106)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:395)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:386)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:473)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-7c47f30ec9ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2867.showString.\n: java.lang.RuntimeException: quote cannot be more than one character\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.getChar(CSVOptions.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:106)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:395)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:386)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:473)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.read.option(\"header\",True).csv(\"ratings_small.csv\")\n",
    "ratings.printSchema()\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp_unix: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "+------+-------+------+--------------+-------------------+\n",
      "|userId|movieId|rating|timestamp_unix|          timestamp|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "|     1|      1|   4.0|     964982703|2000-07-31 01:45:03|\n",
      "|     1|      3|   4.0|     964981247|2000-07-31 01:20:47|\n",
      "|     1|      6|   4.0|     964982224|2000-07-31 01:37:04|\n",
      "|     1|     47|   5.0|     964983815|2000-07-31 02:03:35|\n",
      "|     1|     50|   5.0|     964982931|2000-07-31 01:48:51|\n",
      "|     1|     70|   3.0|     964982400|2000-07-31 01:40:00|\n",
      "|     1|    101|   5.0|     964980868|2000-07-31 01:14:28|\n",
      "|     1|    110|   4.0|     964982176|2000-07-31 01:36:16|\n",
      "|     1|    151|   5.0|     964984041|2000-07-31 02:07:21|\n",
      "|     1|    157|   5.0|     964984100|2000-07-31 02:08:20|\n",
      "|     1|    163|   5.0|     964983650|2000-07-31 02:00:50|\n",
      "|     1|    216|   5.0|     964981208|2000-07-31 01:20:08|\n",
      "|     1|    223|   3.0|     964980985|2000-07-31 01:16:25|\n",
      "|     1|    231|   5.0|     964981179|2000-07-31 01:19:39|\n",
      "|     1|    235|   4.0|     964980908|2000-07-31 01:15:08|\n",
      "|     1|    260|   5.0|     964981680|2000-07-31 01:28:00|\n",
      "|     1|    296|   3.0|     964982967|2000-07-31 01:49:27|\n",
      "|     1|    316|   3.0|     964982310|2000-07-31 01:38:30|\n",
      "|     1|    333|   5.0|     964981179|2000-07-31 01:19:39|\n",
      "|     1|    349|   4.0|     964982563|2000-07-31 01:42:43|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.withColumnRenamed(\"timestamp\",\"timestamp_unix\").withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    "ratings.printSchema()\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop(\"timestamp_unix\",\"foobar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      1|   4.0|2000-07-31 01:45:03|\n",
      "|     1|      3|   4.0|2000-07-31 01:20:47|\n",
      "|     1|      6|   4.0|2000-07-31 01:37:04|\n",
      "|     1|     47|   5.0|2000-07-31 02:03:35|\n",
      "|     1|     50|   5.0|2000-07-31 01:48:51|\n",
      "|     1|     70|   3.0|2000-07-31 01:40:00|\n",
      "|     1|    101|   5.0|2000-07-31 01:14:28|\n",
      "|     1|    110|   4.0|2000-07-31 01:36:16|\n",
      "|     1|    151|   5.0|2000-07-31 02:07:21|\n",
      "|     1|    157|   5.0|2000-07-31 02:08:20|\n",
      "|     1|    163|   5.0|2000-07-31 02:00:50|\n",
      "|     1|    216|   5.0|2000-07-31 01:20:08|\n",
      "|     1|    223|   3.0|2000-07-31 01:16:25|\n",
      "|     1|    231|   5.0|2000-07-31 01:19:39|\n",
      "|     1|    235|   4.0|2000-07-31 01:15:08|\n",
      "|     1|    260|   5.0|2000-07-31 01:28:00|\n",
      "|     1|    296|   3.0|2000-07-31 01:49:27|\n",
      "|     1|    316|   3.0|2000-07-31 01:38:30|\n",
      "|     1|    333|   5.0|2000-07-31 01:19:39|\n",
      "|     1|    349|   4.0|2000-07-31 01:42:43|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|userId|count_user|\n",
      "+------+----------+\n",
      "|     1|       232|\n",
      "|    10|       140|\n",
      "|   100|       148|\n",
      "|   101|        61|\n",
      "|   102|        56|\n",
      "|   103|       377|\n",
      "|   104|       273|\n",
      "|   105|       722|\n",
      "|   106|        33|\n",
      "|   107|        34|\n",
      "|   108|        76|\n",
      "|   109|       127|\n",
      "|    11|        64|\n",
      "|   110|        51|\n",
      "|   111|       646|\n",
      "|   112|        65|\n",
      "|   113|       150|\n",
      "|   114|        31|\n",
      "|   115|       112|\n",
      "|   116|        87|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.groupBy(\"userId\").agg(f.count('userId').alias('count_user')).sort(\"userId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|userId|min_rating|\n",
      "+------+----------+\n",
      "|     1|       1.0|\n",
      "|    10|       0.5|\n",
      "|   100|       1.0|\n",
      "|   101|       1.0|\n",
      "|   102|       1.0|\n",
      "|   103|       0.5|\n",
      "|   104|       0.5|\n",
      "|   105|       0.5|\n",
      "|   106|       2.5|\n",
      "|   107|       3.0|\n",
      "|   108|       1.0|\n",
      "|   109|       2.0|\n",
      "|    11|       1.0|\n",
      "|   110|       1.0|\n",
      "|   111|       0.5|\n",
      "|   112|       0.5|\n",
      "|   113|       1.0|\n",
      "|   114|       0.5|\n",
      "|   115|       1.0|\n",
      "|   116|       0.5|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.groupBy(\"userId\").agg(f.min('rating').alias(\"min_rating\")).sort(\"userId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3027.showString.\n: java.lang.RuntimeException: quote cannot be more than one character\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.getChar(CSVOptions.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:106)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:395)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:386)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:473)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-285-1abac7e65c13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"movies_small.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\"\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"movieId INT, title STRING, genres STRING\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3027.showString.\n: java.lang.RuntimeException: quote cannot be more than one character\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.getChar(CSVOptions.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:106)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:395)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:386)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:473)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "movies = (spark.read.csv(path= \"movies_small.csv\",sep=\",\",header=True,quote='\"\"',schema=\"movieId INT, title STRING, genres STRING\",))\n",
    "movies.printSchema()\n",
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = spark.read.option(\"header\",True).csv(\"movies_small.csv\")\n",
    "movies.printSchema()\n",
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------+------+\n",
      "|movieId|title                                                      |genres|\n",
      "+-------+-----------------------------------------------------------+------+\n",
      "|9      |Sudden Death (1995)                                        |Action|\n",
      "|71     |Fair Game (1995)                                           |Action|\n",
      "|204    |Under Siege 2: Dark Territory (1995)                       |Action|\n",
      "|251    |Hunted, The (1995)                                         |Action|\n",
      "|667    |Bloodsport 2 (a.k.a. Bloodsport II: The Next Kumite) (1996)|Action|\n",
      "+-------+-----------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.where(f.col(\"genres\") == \"Action\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------+------+\n",
      "|movieId|title                                                      |genres|\n",
      "+-------+-----------------------------------------------------------+------+\n",
      "|9      |Sudden Death (1995)                                        |Action|\n",
      "|71     |Fair Game (1995)                                           |Action|\n",
      "|204    |Under Siege 2: Dark Territory (1995)                       |Action|\n",
      "|251    |Hunted, The (1995)                                         |Action|\n",
      "|667    |Bloodsport 2 (a.k.a. Bloodsport II: The Next Kumite) (1996)|Action|\n",
      "+-------+-----------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.filter(\"genres == 'Action'\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|        genres_array|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|[Adventure, Anima...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|[Adventure, Child...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|   [Comedy, Romance]|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|[Comedy, Drama, R...|\n",
      "|      5|Father of the Bri...|              Comedy|            [Comedy]|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|[Action, Crime, T...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|   [Comedy, Romance]|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|[Adventure, Child...|\n",
      "|      9| Sudden Death (1995)|              Action|            [Action]|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|[Action, Adventur...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|[Comedy, Drama, R...|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|    [Comedy, Horror]|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|[Adventure, Anima...|\n",
      "|     14|        Nixon (1995)|               Drama|             [Drama]|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|[Action, Adventur...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|      [Crime, Drama]|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|    [Drama, Romance]|\n",
      "|     18|   Four Rooms (1995)|              Comedy|            [Comedy]|\n",
      "|     19|Ace Ventura: When...|              Comedy|            [Comedy]|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|[Action, Comedy, ...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumn(\"genres_array\",f.split(f.col(\"genres\"),\"\\|\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------+-------------------------------------------------+---------+\n",
      "|movieId|title                             |genres                                     |genres_array                                     |genre    |\n",
      "+-------+----------------------------------+-------------------------------------------+-------------------------------------------------+---------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Adventure|\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Animation|\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Children |\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Comedy   |\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Fantasy  |\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |[Adventure, Children, Fantasy]                   |Adventure|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |[Adventure, Children, Fantasy]                   |Children |\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |[Adventure, Children, Fantasy]                   |Fantasy  |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |[Comedy, Romance]                                |Comedy   |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |[Comedy, Romance]                                |Romance  |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |[Comedy, Drama, Romance]                         |Comedy   |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |[Comedy, Drama, Romance]                         |Drama    |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |[Comedy, Drama, Romance]                         |Romance  |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |[Comedy]                                         |Comedy   |\n",
      "|6      |Heat (1995)                       |Action|Crime|Thriller                      |[Action, Crime, Thriller]                        |Action   |\n",
      "+-------+----------------------------------+-------------------------------------------+-------------------------------------------------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumn(\"genres_array\", f.split(f.col(\"genres\"),\"\\|\")).withColumn(\"genre\",f.explode(\"genres_array\")).show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------+-------------------------------------------------+----------+\n",
      "|movieId|title                             |genres                                     |genres_array                                     |Last_Genre|\n",
      "+-------+----------------------------------+-------------------------------------------+-------------------------------------------------+----------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|[Adventure, Animation, Children, Comedy, Fantasy]|Fantasy   |\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |[Adventure, Children, Fantasy]                   |Fantasy   |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |[Comedy, Romance]                                |Romance   |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |[Comedy, Drama, Romance]                         |Romance   |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |[Comedy]                                         |Comedy    |\n",
      "|6      |Heat (1995)                       |Action|Crime|Thriller                      |[Action, Crime, Thriller]                        |Thriller  |\n",
      "|7      |Sabrina (1995)                    |Comedy|Romance                             |[Comedy, Romance]                                |Romance   |\n",
      "|8      |Tom and Huck (1995)               |Adventure|Children                         |[Adventure, Children]                            |Children  |\n",
      "|9      |Sudden Death (1995)               |Action                                     |[Action]                                         |Action    |\n",
      "|10     |GoldenEye (1995)                  |Action|Adventure|Thriller                  |[Action, Adventure, Thriller]                    |Thriller  |\n",
      "|11     |American President, The (1995)    |Comedy|Drama|Romance                       |[Comedy, Drama, Romance]                         |Romance   |\n",
      "|12     |Dracula: Dead and Loving It (1995)|Comedy|Horror                              |[Comedy, Horror]                                 |Horror    |\n",
      "|13     |Balto (1995)                      |Adventure|Animation|Children               |[Adventure, Animation, Children]                 |Children  |\n",
      "|14     |Nixon (1995)                      |Drama                                      |[Drama]                                          |Drama     |\n",
      "|15     |Cutthroat Island (1995)           |Action|Adventure|Romance                   |[Action, Adventure, Romance]                     |Romance   |\n",
      "+-------+----------------------------------+-------------------------------------------+-------------------------------------------------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.withColumn(\"genres_array\",f.split(\"genres\",\"\\|\")).withColumn(\"Last_Genre\",f.element_at(f.col(\"genres_array\"),-1)).show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- imdbId: string (nullable = true)\n",
      " |-- tmdbId: string (nullable = true)\n",
      "\n",
      "+-------+-------+------+\n",
      "|movieId| imdbId|tmdbId|\n",
      "+-------+-------+------+\n",
      "|      1|0114709|   862|\n",
      "|      2|0113497|  8844|\n",
      "|      3|0113228| 15602|\n",
      "|      4|0114885| 31357|\n",
      "|      5|0113041| 11862|\n",
      "|      6|0113277|   949|\n",
      "|      7|0114319| 11860|\n",
      "|      8|0112302| 45325|\n",
      "|      9|0114576|  9091|\n",
      "|     10|0113189|   710|\n",
      "|     11|0112346|  9087|\n",
      "|     12|0112896| 12110|\n",
      "|     13|0112453| 21032|\n",
      "|     14|0113987| 10858|\n",
      "|     15|0112760|  1408|\n",
      "|     16|0112641|   524|\n",
      "|     17|0114388|  4584|\n",
      "|     18|0113101|     5|\n",
      "|     19|0112281|  9273|\n",
      "|     20|0113845| 11517|\n",
      "+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "+------+-------+-----------------+-------------------+\n",
      "|userId|movieId|              tag|          timestamp|\n",
      "+------+-------+-----------------+-------------------+\n",
      "|     2|  60756|            funny|2015-10-25 02:29:54|\n",
      "|     2|  60756|  Highly quotable|2015-10-25 02:29:56|\n",
      "|     2|  60756|     will ferrell|2015-10-25 02:29:52|\n",
      "|     2|  89774|     Boxing story|2015-10-25 02:33:27|\n",
      "|     2|  89774|              MMA|2015-10-25 02:33:20|\n",
      "|     2|  89774|        Tom Hardy|2015-10-25 02:33:25|\n",
      "|     2| 106782|            drugs|2015-10-25 02:30:54|\n",
      "|     2| 106782|Leonardo DiCaprio|2015-10-25 02:30:51|\n",
      "|     2| 106782|  Martin Scorsese|2015-10-25 02:30:56|\n",
      "|     7|  48516|     way too long|2007-01-25 08:08:45|\n",
      "|    18|    431|        Al Pacino|2016-05-02 04:39:25|\n",
      "|    18|    431|         gangster|2016-05-02 04:39:09|\n",
      "|    18|    431|            mafia|2016-05-02 04:39:15|\n",
      "|    18|   1221|        Al Pacino|2016-04-27 02:35:06|\n",
      "|    18|   1221|            Mafia|2016-04-27 02:35:03|\n",
      "|    18|   5995|        holocaust|2016-02-18 01:57:52|\n",
      "|    18|   5995|       true story|2016-02-18 01:57:59|\n",
      "|    18|  44665|     twist ending|2016-03-03 02:51:23|\n",
      "|    18|  52604|  Anthony Hopkins|2016-03-11 05:58:16|\n",
      "|    18|  52604|  courtroom drama|2016-03-11 05:58:31|\n",
      "+------+-------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links = spark.read.option(\"header\",True).csv(\"links_small.csv\")\n",
    "links.printSchema()\n",
    "links.show()\n",
    "tags = spark.read.option(\"header\",True).csv(\"tags_small.csv\").withColumn(\"timestamp\",f.to_timestamp(f.from_unixtime(\"timestamp\")))\n",
    "tags.printSchema()\n",
    "tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+-------+----------------+-------------------+\n",
      "|movieId|               title|              genres|userId|movieId|             tag|          timestamp|\n",
      "+-------+--------------------+--------------------+------+-------+----------------+-------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|   567|      1|             fun|2018-05-03 01:33:33|\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|   474|      1|           pixar|2006-01-14 09:47:05|\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|   336|      1|           pixar|2006-02-04 16:36:04|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|   474|      2|            game|2006-01-16 08:39:12|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|    62|      2|  Robin Williams|2018-06-13 05:51:47|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|    62|      2|magic board game|2018-06-13 05:52:12|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|    62|      2|         fantasy|2018-06-13 05:52:09|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|   289|      3|             old|2006-03-27 09:01:00|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|   289|      3|           moldy|2006-03-27 09:01:00|\n",
      "|      5|Father of the Bri...|              Comedy|   474|      5|          remake|2006-01-16 08:11:43|\n",
      "|      5|Father of the Bri...|              Comedy|   474|      5|       pregnancy|2006-01-16 08:11:43|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|   474|      7|          remake|2006-01-16 08:40:42|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|   474|     11|       president|2006-01-16 08:28:24|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|   474|     11|        politics|2006-01-16 08:28:24|\n",
      "|     14|        Nixon (1995)|               Drama|   474|     14|       president|2006-01-16 08:40:23|\n",
      "|     14|        Nixon (1995)|               Drama|   474|     14|        politics|2006-01-16 08:40:23|\n",
      "|     16|       Casino (1995)|         Crime|Drama|   474|     16|           Mafia|2006-01-14 02:47:20|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|   474|     17|     Jane Austen|2006-01-14 02:39:13|\n",
      "|     21|   Get Shorty (1995)|Comedy|Crime|Thri...|   474|     21|       Hollywood|2006-01-14 09:36:18|\n",
      "|     22|      Copycat (1995)|Crime|Drama|Horro...|   474|     22|   serial killer|2006-01-16 08:38:16|\n",
      "+-------+--------------------+--------------------+------+-------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opinions = movies.join(tags,movies[\"movieId\"] == tags[\"movieId\"])\n",
    "opinions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+------+----------+-------------------+\n",
      "|movieId|           title|              genres|userId|       tag|          timestamp|\n",
      "+-------+----------------+--------------------+------+----------+-------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   474|     pixar|2006-01-14 09:47:05|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   567|       fun|2018-05-03 01:33:33|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   336|     pixar|2006-02-04 16:36:04|\n",
      "| 100083| Movie 43 (2013)|              Comedy|   125|   sarcasm|2016-09-20 20:10:15|\n",
      "| 100083| Movie 43 (2013)|              Comedy|   125|R language|2016-09-20 20:10:35|\n",
      "+-------+----------------+--------------------+------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+----------------+--------------------+------+-----+-------------------+\n",
      "|movieId|           title|              genres|userId|  tag|          timestamp|\n",
      "+-------+----------------+--------------------+------+-----+-------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   336|pixar|2006-02-04 16:36:04|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   567|  fun|2018-05-03 01:33:33|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   474|pixar|2006-01-14 09:47:05|\n",
      "|     10|GoldenEye (1995)|Action|Adventure|...|  null| null|               null|\n",
      "|    100|City Hall (1996)|      Drama|Thriller|  null| null|               null|\n",
      "+-------+----------------+--------------------+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+----------------+--------------------+------+-----+-------------------+\n",
      "|movieId|           title|              genres|userId|  tag|          timestamp|\n",
      "+-------+----------------+--------------------+------+-----+-------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   474|pixar|2006-01-14 09:47:05|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   567|  fun|2018-05-03 01:33:33|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   336|pixar|2006-02-04 16:36:04|\n",
      "|     10|GoldenEye (1995)|Action|Adventure|...|  null| null|               null|\n",
      "|    100|City Hall (1996)|      Drama|Thriller|  null| null|               null|\n",
      "+-------+----------------+--------------------+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+----------------+--------------------+------+------------------+-------------------+\n",
      "|movieId|           title|              genres|userId|               tag|          timestamp|\n",
      "+-------+----------------+--------------------+------+------------------+-------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   474|             pixar|2006-01-14 09:47:05|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   336|             pixar|2006-02-04 16:36:04|\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|   567|               fun|2018-05-03 01:33:33|\n",
      "| 100083| Movie 43 (2013)|              Comedy|   125|embarassing scenes|2016-09-20 20:11:15|\n",
      "| 100083| Movie 43 (2013)|              Comedy|   125|           sarcasm|2016-09-20 20:10:15|\n",
      "+-------+----------------+--------------------+------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.join(tags,[\"movieId\"],\"inner\").sort('movieID').show(5)\n",
    "movies.join(tags,[\"movieId\"],\"outer\").sort('movieID').show(5)\n",
    "movies.join(tags,[\"movieId\"],\"left\").sort('movieID').show(5)\n",
    "movies.join(tags,[\"movieId\"],\"right\").sort('movieID').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      1|   4.0|2000-07-31 01:45:03|\n",
      "|     1|      3|   4.0|2000-07-31 01:20:47|\n",
      "|     1|      6|   4.0|2000-07-31 01:37:04|\n",
      "|     1|     47|   5.0|2000-07-31 02:03:35|\n",
      "|     1|     50|   5.0|2000-07-31 01:48:51|\n",
      "|     1|     70|   3.0|2000-07-31 01:40:00|\n",
      "|     1|    101|   5.0|2000-07-31 01:14:28|\n",
      "|     1|    110|   4.0|2000-07-31 01:36:16|\n",
      "|     1|    151|   5.0|2000-07-31 02:07:21|\n",
      "|     1|    157|   5.0|2000-07-31 02:08:20|\n",
      "|     1|    163|   5.0|2000-07-31 02:00:50|\n",
      "|     1|    216|   5.0|2000-07-31 01:20:08|\n",
      "|     1|    223|   3.0|2000-07-31 01:16:25|\n",
      "|     1|    231|   5.0|2000-07-31 01:19:39|\n",
      "|     1|    235|   4.0|2000-07-31 01:15:08|\n",
      "|     1|    260|   5.0|2000-07-31 01:28:00|\n",
      "|     1|    296|   3.0|2000-07-31 01:49:27|\n",
      "|     1|    316|   3.0|2000-07-31 01:38:30|\n",
      "|     1|    333|   5.0|2000-07-31 01:19:39|\n",
      "|     1|    349|   4.0|2000-07-31 01:42:43|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+--------------------+--------------------+----------------+-------------------+------+-------------------+\n",
      "|userId|movieId|               title|              genres|             tag|          timestamp|rating|          timestamp|\n",
      "+------+-------+--------------------+--------------------+----------------+-------------------+------+-------------------+\n",
      "|   567|      1|    Toy Story (1995)|Adventure|Animati...|             fun|2018-05-03 01:33:33|   3.5|2018-05-03 01:33:21|\n",
      "|   474|      1|    Toy Story (1995)|Adventure|Animati...|           pixar|2006-01-14 09:47:05|   4.0|2001-01-04 09:36:00|\n",
      "|   336|      1|    Toy Story (1995)|Adventure|Animati...|           pixar|2006-02-04 16:36:04|   4.0|2005-07-25 00:48:49|\n",
      "|   474|      2|      Jumanji (1995)|Adventure|Childre...|            game|2006-01-16 08:39:12|   3.0|2003-03-06 00:53:34|\n",
      "|    62|      2|      Jumanji (1995)|Adventure|Childre...|  Robin Williams|2018-06-13 05:51:47|   4.0|2018-06-13 05:51:30|\n",
      "|    62|      2|      Jumanji (1995)|Adventure|Childre...|magic board game|2018-06-13 05:52:12|   4.0|2018-06-13 05:51:30|\n",
      "|    62|      2|      Jumanji (1995)|Adventure|Childre...|         fantasy|2018-06-13 05:52:09|   4.0|2018-06-13 05:51:30|\n",
      "|   289|      3|Grumpier Old Men ...|      Comedy|Romance|             old|2006-03-27 09:01:00|   2.5|2006-03-27 08:57:37|\n",
      "|   289|      3|Grumpier Old Men ...|      Comedy|Romance|           moldy|2006-03-27 09:01:00|   2.5|2006-03-27 08:57:37|\n",
      "|   474|      5|Father of the Bri...|              Comedy|          remake|2006-01-16 08:11:43|   1.5|2003-05-16 01:06:22|\n",
      "+------+-------+--------------------+--------------------+----------------+-------------------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()\n",
    "movies.join(tags,[\"movieId\"],\"inner\").join(ratings,[\"userId\",\"movieId\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+--------------------+----------------+-------------------+------+-------------------+\n",
      "|movieId|userId|               title|              genres|             tag|      tag_timestamp|rating|          timestamp|\n",
      "+-------+------+--------------------+--------------------+----------------+-------------------+------+-------------------+\n",
      "|      1|   567|    Toy Story (1995)|Adventure|Animati...|             fun|2018-05-03 01:33:33|   3.5|2018-05-03 01:33:21|\n",
      "|      1|   474|    Toy Story (1995)|Adventure|Animati...|           pixar|2006-01-14 09:47:05|   4.0|2001-01-04 09:36:00|\n",
      "|      1|   336|    Toy Story (1995)|Adventure|Animati...|           pixar|2006-02-04 16:36:04|   4.0|2005-07-25 00:48:49|\n",
      "|      2|   474|      Jumanji (1995)|Adventure|Childre...|            game|2006-01-16 08:39:12|   3.0|2003-03-06 00:53:34|\n",
      "|      2|    62|      Jumanji (1995)|Adventure|Childre...|  Robin Williams|2018-06-13 05:51:47|   4.0|2018-06-13 05:51:30|\n",
      "|      2|    62|      Jumanji (1995)|Adventure|Childre...|magic board game|2018-06-13 05:52:12|   4.0|2018-06-13 05:51:30|\n",
      "|      2|    62|      Jumanji (1995)|Adventure|Childre...|         fantasy|2018-06-13 05:52:09|   4.0|2018-06-13 05:51:30|\n",
      "|      3|   289|Grumpier Old Men ...|      Comedy|Romance|             old|2006-03-27 09:01:00|   2.5|2006-03-27 08:57:37|\n",
      "|      3|   289|Grumpier Old Men ...|      Comedy|Romance|           moldy|2006-03-27 09:01:00|   2.5|2006-03-27 08:57:37|\n",
      "|      5|   474|Father of the Bri...|              Comedy|          remake|2006-01-16 08:11:43|   1.5|2003-05-16 01:06:22|\n",
      "|      5|   474|Father of the Bri...|              Comedy|       pregnancy|2006-01-16 08:11:43|   1.5|2003-05-16 01:06:22|\n",
      "|      7|   474|      Sabrina (1995)|      Comedy|Romance|          remake|2006-01-16 08:40:42|   3.0|2001-01-04 09:46:21|\n",
      "|     11|   474|American Presiden...|Comedy|Drama|Romance|       president|2006-01-16 08:28:24|   2.5|2003-05-16 00:57:17|\n",
      "|     11|   474|American Presiden...|Comedy|Drama|Romance|        politics|2006-01-16 08:28:24|   2.5|2003-05-16 00:57:17|\n",
      "|     14|   474|        Nixon (1995)|               Drama|       president|2006-01-16 08:40:23|   3.0|2005-07-08 19:54:07|\n",
      "|     14|   474|        Nixon (1995)|               Drama|        politics|2006-01-16 08:40:23|   3.0|2005-07-08 19:54:07|\n",
      "|     16|   474|       Casino (1995)|         Crime|Drama|           Mafia|2006-01-14 02:47:20|   4.0|2004-06-28 19:45:31|\n",
      "|     17|   474|Sense and Sensibi...|       Drama|Romance|     Jane Austen|2006-01-14 02:39:13|   5.0|2000-11-20 04:17:46|\n",
      "|     21|   474|   Get Shorty (1995)|Comedy|Crime|Thri...|       Hollywood|2006-01-14 09:36:18|   4.0|2005-06-20 08:59:44|\n",
      "|     22|   474|      Copycat (1995)|Crime|Drama|Horro...|   serial killer|2006-01-16 08:38:16|   3.0|2003-03-06 03:26:46|\n",
      "+-------+------+--------------------+--------------------+----------------+-------------------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.join(tags,[\"movieId\"],\"inner\").withColumnRenamed(\"timestamp\",\"tag_timestamp\").join(ratings,[\"movieId\",\"userId\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+-----------+------------------+-------------------+-------------------+\n",
      "|movieId|count|min(rating)|max(rating)|       avg(rating)|     min(timestamp)|     max(timestamp)|\n",
      "+-------+-----+-----------+-----------+------------------+-------------------+-------------------+\n",
      "| 100553|    2|        4.5|        4.5|               4.5|2015-03-15 00:14:07|2015-11-04 00:39:58|\n",
      "| 102684|    2|        3.5|        4.0|              3.75|2013-08-16 19:02:32|2017-05-14 03:59:59|\n",
      "|   1090|   63|        1.0|        5.0| 3.984126984126984|1997-03-19 18:50:39|2018-08-11 07:54:18|\n",
      "| 112911|    4|        0.5|        4.0|               2.0|2015-12-18 19:26:23|2018-05-03 02:22:29|\n",
      "| 115713|   28|        0.5|        5.0|3.9107142857142856|2015-05-22 20:18:30|2018-09-17 11:20:30|\n",
      "| 117630|    1|        1.0|        1.0|               1.0|2018-02-23 15:03:51|2018-02-23 15:03:51|\n",
      "| 119655|    2|        1.0|        3.5|              2.25|2016-03-29 00:38:32|2016-04-05 01:10:44|\n",
      "| 120478|    3|        3.5|        5.0| 4.333333333333333|2016-08-09 07:31:31|2018-08-23 22:20:21|\n",
      "| 121007|    1|        4.0|        4.0|               4.0|2018-03-07 14:49:28|2018-03-07 14:49:28|\n",
      "|   1572|    2|        2.5|        3.5|               3.0|2005-09-05 01:09:05|2007-07-30 13:38:04|\n",
      "| 158813|    4|        1.0|        3.0|               2.0|2016-05-27 00:11:12|2018-02-12 03:28:03|\n",
      "| 173535|    1|        4.5|        4.5|               4.5|2018-05-13 17:23:13|2018-05-13 17:23:13|\n",
      "|   2069|    2|        3.5|        5.0|              4.25|2003-03-31 09:02:30|2005-12-19 04:49:09|\n",
      "|   2088|   18|        1.0|        4.0|               2.5|1999-11-03 22:04:32|2018-08-13 05:51:42|\n",
      "|   2136|   14|        0.5|        5.0|2.4642857142857144|1999-10-13 17:37:12|2012-02-02 08:19:04|\n",
      "|   2162|    8|        1.0|        3.5|               2.5|2000-10-15 21:22:34|2018-02-22 00:29:27|\n",
      "|   2294|   45|        1.5|        5.0|3.2444444444444445|1999-02-28 18:27:31|2018-09-02 03:30:29|\n",
      "|  26082|    3|        4.0|        5.0|               4.5|2006-10-26 14:56:24|2015-11-04 00:52:09|\n",
      "|  27317|    6|        3.0|        4.5|              3.75|2005-04-05 21:57:39|2017-06-30 12:23:18|\n",
      "|    296|  307|        0.5|        5.0| 4.197068403908795|1996-04-18 00:08:18|2018-09-17 11:12:07|\n",
      "+-------+-----+-----------+-----------+------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.groupBy(\"movieId\").agg(\n",
    "    f.count(\"*\").alias(\"count\"),\n",
    "    f.min(\"rating\"),\n",
    "    f.max(\"rating\"),\n",
    "    f.avg(\"rating\"),\n",
    "    f.min(\"timestamp\"),\n",
    "    f.max(\"timestamp\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+-------------+-------------------+-------------------+\n",
      "|movieId|    collect_set(tag)|count(tag)| collect_set(userId)|count(userId)|     min(timestamp)|     max(timestamp)|\n",
      "+-------+--------------------+----------+--------------------+-------------+-------------------+-------------------+\n",
      "|   1090|           [Vietnam]|         1|               [474]|            1|2006-01-25 04:26:30|2006-01-25 04:26:30|\n",
      "|    296|[foul language, r...|       181|[474, 103, 424, 599]|          181|2006-01-14 09:16:41|2017-06-26 12:58:14|\n",
      "|   3210|       [high school]|         1|               [474]|            1|2006-01-16 08:19:41|2006-01-16 08:19:41|\n",
      "|   1372|          [Klingons]|         1|               [474]|            1|2006-01-14 08:44:24|2006-01-14 08:44:24|\n",
      "|   1394|     [Coen Brothers]|         1|               [474]|            1|2006-01-14 02:43:35|2006-01-14 02:43:35|\n",
      "|   2393|[space opera, Sta...|         3|               [477]|            3|2009-05-14 12:21:03|2009-05-14 12:21:13|\n",
      "|   3281|         [sexuality]|         1|               [474]|            1|2006-01-16 08:18:50|2006-01-16 08:18:50|\n",
      "|  58559|[gritty, psycholo...|         4|          [435, 567]|            4|2013-04-23 07:14:00|2018-05-03 01:26:41|\n",
      "|    800|  [In Netflix queue]|         1|               [474]|            1|2006-01-14 08:26:14|2006-01-14 08:26:14|\n",
      "|  90439|[Kevin Spacey, Wa...|         9|               [537]|            9|2015-02-17 09:03:55|2015-02-17 09:44:19|\n",
      "+-------+--------------------+----------+--------------------+-------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags.groupBy(\"movieId\").agg(\n",
    "    f.collect_set(\"tag\"),\n",
    "    f.count(\"tag\"),\n",
    "    f.collect_set(\"userId\"),\n",
    "    f.count(\"userId\"),\n",
    "    f.min(\"timestamp\"),\n",
    "    f.max(\"timestamp\")\n",
    ").show(10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame representing the stream of input lines from connecttion to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream\\\n",
    "    .format(\"socket\")\\\n",
    "    .option(\"host\",\"localhost\")\\\n",
    "    .option(\"port\", 9999)\\\n",
    "    .load()\n",
    "\n",
    "#split the lines into words\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value,\" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "#generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o46.start.\n: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\vannhinh.ng\\AppData\\Local\\Temp\\temporary-013d6f62-064b-4f40-bb6a-4869c3287a9b\\.metadata.b7777a7d-4ace-4bf8-bb0a-b831ec84964a.tmp\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1017)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:596)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:688)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:177)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:175)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:317)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:359)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:367)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-594588fe594b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Start running the query that prints the running counts to the console\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordCounts\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"console\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\streaming.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o46.start.\n: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\vannhinh.ng\\AppData\\Local\\Temp\\temporary-013d6f62-064b-4f40-bb6a-4869c3287a9b\\.metadata.b7777a7d-4ace-4bf8-bb0a-b831ec84964a.tmp\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1017)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:596)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:688)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:177)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:175)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:317)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:359)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:367)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o51.start.\n: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\vannhinh.ng\\AppData\\Local\\Temp\\temporary-43f8e3fd-6381-41a1-830c-4b56b381a6d8\\.metadata.df70cff1-655a-4c70-84d8-9bbdfbed9543.tmp\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1017)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:596)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:688)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:177)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:175)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:317)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:359)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:303)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-da9386528e19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Start running the query that write the running counts to memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordCounts\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wordCounts\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\streaming.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o51.start.\n: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\vannhinh.ng\\AppData\\Local\\Temp\\temporary-43f8e3fd-6381-41a1-830c-4b56b381a6d8\\.metadata.df70cff1-655a-4c70-84d8-9bbdfbed9543.tmp\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1017)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:596)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:688)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:177)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:175)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:317)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:359)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:303)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "#Start running the query that write the running counts to memory\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"wordCounts\")\\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ddc7ebe037fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"SELECT * from {query.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "display(spark.sql(f\"SELECT * from {query.name}\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0edbc16d4780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#spark.sql can be used to request how the query is performing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"SELECT * from {query.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "#show live results for 2 minutes, refreshed every 1 second\n",
    "from time import sleep\n",
    "for x in range(0,120):\n",
    "    #spark.sql can be used to request how the query is performing\n",
    "    display(spark.sql(f\"SELECT * from {query.name}\").toPandas())\n",
    "    sleep(1)\n",
    "    clear_output(wait=True)\n",
    "else:\n",
    "    print(\"Live view ended...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "import html\n",
    "\n",
    "# Settings\n",
    "\n",
    "#IN_PATH = \"/kaggle/input/twitter-data-for-spark-streaming/\"\n",
    "IN_PATH = \"./TwitterStreaming/\"\n",
    "\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzzz yyyy\"\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingExample\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "schema = spark.read.json(IN_PATH).limit(10).schema\n",
    "\n",
    "spark_reader = spark.readStream.schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-31af859db85b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mstreaming_data_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreaming_data_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mstream_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstreaming_data_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"append\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-d501e51b307c>\u001b[0m in \u001b[0;36mclean_data\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"@[A-Za-z0-9]+\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Remove @ sign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"(?:\\@|http?\\://|https?\\://|www)\\S+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Remove http links\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "streaming_data_raw = (\n",
    "    spark_reader.json(IN_PATH)\n",
    "    .select(\n",
    "        \"id\",\n",
    "        #extract proper timestamp from created at column\n",
    "        f.to_timestamp(f.col(\"created_at\"), timestampformat).alias(\"timestamp\"),\n",
    "        #extract user information\n",
    "        f.col(\"user.screen_name\").alias(\"user\"),\n",
    "        \"text\",\n",
    "    )\n",
    "    .coalesce(1)\n",
    ")\n",
    "streaming_data_clean = clean_data(streaming_data_raw)\n",
    "\n",
    "stream_writer = (streaming_data_clean.writeStream.queryName(\"data\").trigger(once=True).outputMode(\"append\").format(\"memory\"))\n",
    "\n",
    "query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ddc7ebe037fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"SELECT * from {query.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "display(spark.sql(f\"SELECT * from {query.name}\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'streaming_data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-982561c9f2d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdistinct_user_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstreaming_data_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapprox_count_distinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_timestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstream_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdistinct_user_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'streaming_data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "distinct_user_count = streaming_data_clean.select(f.approx_count_distinct(\"user\"), f.current_timestamp())\n",
    "\n",
    "stream_writer = (distinct_user_count.writeStream.queryName(\"data\").trigger(once=True).outputMode(\"complete\").format(\"memory\"))\n",
    "\n",
    "query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ddc7ebe037fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"SELECT * from {query.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "display(spark.sql(f\"SELECT * from {query.name}\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o66.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/kaggle/input/pyspark-nlp/MODEL/metadata\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-da59c95ae808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentiment_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/pyspark-nlp/MODEL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw_sentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreaming_data_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#select downstream columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m sentiment = raw_sentiment.select(\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'language'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paramMap'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paramMap'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaMLReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mloadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \"\"\"\n\u001b[0;32m    503\u001b[0m         \u001b[0mmetadataPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[0mmetadataStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[0mloadedVals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parseMetaData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataStr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpectedClassName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloadedVals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1462\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m         \"\"\"\n\u001b[1;32m-> 1464\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1465\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1411\u001b[0m         \"\"\"\n\u001b[0;32m   1412\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1414\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o66.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/kaggle/input/pyspark-nlp/MODEL/metadata\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = PipelineModel.load(\"/kaggle/input/pyspark-nlp/MODEL\")\n",
    "raw_sentiment = sentiment_model.transform(streaming_data_clean)\n",
    "\n",
    "#select downstream columns\n",
    "sentiment = raw_sentiment.select(\n",
    "    \"id\", \"timestamp\", \"user\", \"text\", f.col(\"prediction\").alias(\"user_sentiment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-94f98910268f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstream_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"append\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "stream_writer = (sentiment.writeStream.queryName(\"data\").trigger(once=True).outputMode(\"append\").format(\"memory\"))\n",
    "\n",
    "query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ddc7ebe037fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"SELECT * from {query.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "display(spark.sql(f\"SELECT * from {query.name}\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a65cc4220861>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#streaming from files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m negative_sentiment_count = (\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_sentiment == 0.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_sentiment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"negative_sentiment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"negative_sentiment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "#streaming from files\n",
    "negative_sentiment_count = (\n",
    "    sentiment.filter(\"user_sentiment == 0.0\")\n",
    "    .select(f.col(\"user_sentiment\").alias(\"negative_sentiment\"))\n",
    "    .agg(f.count(\"negative_sentiment\"))\n",
    ")\n",
    "\n",
    "positive_sentiment_count = (\n",
    "    sentiment.filter(\"user_sentiment == 4.0\")\n",
    "    .select(f.col(\"user_sentiment\").alias(\"Positive_sentiment\"))\n",
    "    .agg(f.count(\"positive_sentiment\"))\n",
    ")\n",
    "\n",
    "average_sentiment = sentiment.agg(f.avg(\"user_sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7e61a5905199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_to_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'average_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "data_to_stream = average_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4b0bd7e394a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataStreamReader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     stream_writer = (\n\u001b[0;32m      3\u001b[0m         \u001b[0mdata_to_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"streaming_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"20 seconds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m#.trigger(once=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark_reader' is not defined"
     ]
    }
   ],
   "source": [
    "if isinstance(spark_reader, DataStreamReader):\n",
    "    stream_writer = (\n",
    "        data_to_stream.writeStream.queryName(\"streaming_table\")\n",
    "        .trigger(processingTime=\"20 seconds\")\n",
    "        #.trigger(once=True)\n",
    "        .outputMode(\"completed\")\n",
    "        .format(\"memory\")\n",
    "    )\n",
    "    #calling .start on a DataStreamWriter return an instance of StreamingQuery\n",
    "    query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ddc7ebe037fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"SELECT * from {query.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "display(spark.sql(f\"SELECT * from {query.name}\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'streaming_data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9e7877885901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# let's see what we are outputting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mstreaming_data_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misStreaming\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'streaming_data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# let's see what we are outputting\n",
    "if streaming_data_clean.isStreaming:\n",
    "    from time import sleep\n",
    "    for x in range(0,200):\n",
    "        try:\n",
    "            if not query.isActive:\n",
    "                break\n",
    "            print(\"showing live view refreshed every 10 seconds\")\n",
    "            print(f\"Seconds passed: {x*10}\")\n",
    "            result = spark.sql(f\"SELECT * from {query.name}\")\n",
    "            # spark.sql can be used to request how the query is performing\n",
    "            display(result.toPandas())\n",
    "            sleep(10)\n",
    "            clear_output(wait=True)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    print(\"Live view ended...\")\n",
    "else:\n",
    "    print(\"Not streaming, showing static output instead\")\n",
    "    result = data_to_stream\n",
    "    display(result.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#111111111111111111111111111\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "schema = \"polarity Float, id LONG, date_time STRING, query STRING, user STRING, text STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- polarity: float (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_time: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+--------+---+--------------------+-------+--------------+--------------------+\n",
      "|polarity| id|           date_time|  query|          user|                text|\n",
      "+--------+---+--------------------+-------+--------------+--------------------+\n",
      "|     4.0|  3|Mon May 11 03:17:...|kindle2|        tpryan|@stellargirl I lo...|\n",
      "|     4.0|  4|Mon May 11 03:18:...|kindle2|        vcu451|Reading my kindle...|\n",
      "|     4.0|  5|Mon May 11 03:18:...|kindle2|        chadfu|Ok, first assesme...|\n",
      "|     4.0|  6|Mon May 11 03:19:...|kindle2|         SIX15|@kenburbary You'l...|\n",
      "|     4.0|  7|Mon May 11 03:21:...|kindle2|      yamarama|@mikefish  Fair e...|\n",
      "|     4.0|  8|Mon May 11 03:22:...|kindle2|  GeorgeVHulme|@richardebaker no...|\n",
      "|     0.0|  9|Mon May 11 03:22:...|    aig|       Seth937|Fuck this economy...|\n",
      "|     4.0| 10|Mon May 11 03:26:...| jquery|     dcostalis|Jquery is my new ...|\n",
      "|     4.0| 11|Mon May 11 03:27:...|twitter|       PJ_King|       Loves twitter|\n",
      "|     4.0| 12|Mon May 11 03:29:...|  obama|   mandanicole|how can you not l...|\n",
      "|     2.0| 13|Mon May 11 03:32:...|  obama|          jpeb|Check this video ...|\n",
      "|     0.0| 14|Mon May 11 03:32:...|  obama|   kylesellers|@Karoli I firmly ...|\n",
      "|     4.0| 15|Mon May 11 03:33:...|  obama|   theviewfans|House Corresponde...|\n",
      "|     4.0| 16|Mon May 11 05:05:...|   nike|        MumsFP|Watchin Espn..Jus...|\n",
      "|     0.0| 17|Mon May 11 05:06:...|   nike|   vincentx24x|dear nike, stop w...|\n",
      "|     4.0| 18|Mon May 11 05:20:...| lebron|  cameronwylie|#lebron best athl...|\n",
      "|     0.0| 19|Mon May 11 05:20:...| lebron|       luv8242|I was talking to ...|\n",
      "|     4.0| 20|Mon May 11 05:21:...| lebron|    mtgillikin|i love lebron. ht...|\n",
      "|     0.0| 21|Mon May 11 05:21:...| lebron|ursecretdezire|@ludajuice Lebron...|\n",
      "|     4.0| 22|Mon May 11 05:21:...| lebron|     Native_01|@Pmillzz lebron I...|\n",
      "+--------+---+--------------------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_test_data = spark.read.options(delimiter=',').schema(\"polarity Float, id LONG, date_time STRING, query STRING, user STRING, text STRING\").csv(\"testdata.manual.2009.06.14.csv\")\n",
    "raw_test_data.printSchema()\n",
    "raw_test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows with Polarity: 498/498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12fd2b0cc10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = raw_test_data.select(\"polarity\").na.drop()\n",
    "print(f\"No of rows with Polarity: {df.count()}/{raw_test_data.count()}\")\n",
    "sns.distplot(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- polarity: float (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|polarity|                text|\n",
      "+--------+--------------------+\n",
      "|     0.0|@switchfoot http:...|\n",
      "|     0.0|is upset that he ...|\n",
      "|     0.0|@Kenichan I dived...|\n",
      "|     0.0|my whole body fee...|\n",
      "|     0.0|@nationwideclass ...|\n",
      "|     0.0|@Kwesidei not the...|\n",
      "|     0.0|         Need a hug |\n",
      "|     0.0|@LOLTrish hey  lo...|\n",
      "|     0.0|@Tatiana_K nope t...|\n",
      "|     0.0|@twittera que me ...|\n",
      "|     0.0|spring break in p...|\n",
      "|     0.0|I just re-pierced...|\n",
      "|     0.0|@caregiving I cou...|\n",
      "|     0.0|@octolinz16 It it...|\n",
      "|     0.0|@smarrison i woul...|\n",
      "|     0.0|@iamjazzyfizzle I...|\n",
      "|     0.0|Hollis' death sce...|\n",
      "|     0.0|about to file taxes |\n",
      "|     0.0|@LettyA ahh ive a...|\n",
      "|     0.0|@FakerPattyPattz ...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_training_data = spark.read.options(delimiter=',').schema(\"polarity Float, text STRING\").csv(\"training.1600000.processed.noemoticon.csv\")\n",
    "raw_training_data.printSchema()\n",
    "raw_training_data.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows with Polarity: 1048555/1048555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12fd30ea9a0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = raw_training_data.select(\"polarity\").na.drop()\n",
    "print(f\"No of rows with Polarity: {df.count()}/{raw_training_data.count()}\")\n",
    "sns.distplot(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>248576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>799979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity   count\n",
       "0       4.0  248576\n",
       "1       0.0  799979"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polariry_df = raw_training_data.select(\"polarity\").cache()\n",
    "\n",
    "polariry_df.groupBy(\"polarity\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o528.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 30.0 failed 1 times, most recent failure: Lost task 5.0 in stage 30.0 (TID 411, vannhinh-ng02.ea.corp.samsungelectronics.net, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\RAW\\_temporary\\0\\_temporary\\attempt_202201171657426473507541072125269_0030_m_000005_411\\polarity=0.0\\part-00005-8f8eabf8-e160-4db7-8fbf-4d4e781d4c79.c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\RAW\\_temporary\\0\\_temporary\\attempt_202201171657426473507541072125269_0030_m_000005_411\\polarity=0.0\\part-00005-8f8eabf8-e160-4db7-8fbf-4d4e781d4c79.c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-12fbd5aa9642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./RAW/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_training_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"polarity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o528.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 30.0 failed 1 times, most recent failure: Lost task 5.0 in stage 30.0 (TID 411, vannhinh-ng02.ea.corp.samsungelectronics.net, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\RAW\\_temporary\\0\\_temporary\\attempt_202201171657426473507541072125269_0030_m_000005_411\\polarity=0.0\\part-00005-8f8eabf8-e160-4db7-8fbf-4d4e781d4c79.c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 E:\\spark\\spark-3.0.3-bin-hadoop2.7\\BTGK\\RAW\\_temporary\\0\\_temporary\\attempt_202201171657426473507541072125269_0030_m_000005_411\\polarity=0.0\\part-00005-8f8eabf8-e160-4db7-8fbf-4d4e781d4c79.c000.csv\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"./RAW/\"\n",
    "raw_training_data.repartition(20).write.partitionBy(\"polarity\").csv(OUTPUT_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------------+-------+------------+--------------------+\n",
      "|polarity| id|               date|  query|        user|                text|\n",
      "+--------+---+-------------------+-------+------------+--------------------+\n",
      "|     4.0|  3|2009-05-11 10:17:40|kindle2|      tpryan|@stellargirl I lo...|\n",
      "|     4.0|  4|2009-05-11 10:18:03|kindle2|      vcu451|Reading my kindle...|\n",
      "|     4.0|  5|2009-05-11 10:18:54|kindle2|      chadfu|Ok, first assesme...|\n",
      "|     4.0|  6|2009-05-11 10:19:04|kindle2|       SIX15|@kenburbary You'l...|\n",
      "|     4.0|  7|2009-05-11 10:21:41|kindle2|    yamarama|@mikefish  Fair e...|\n",
      "|     4.0|  8|2009-05-11 10:22:00|kindle2|GeorgeVHulme|@richardebaker no...|\n",
      "|     0.0|  9|2009-05-11 10:22:30|    aig|     Seth937|Fuck this economy...|\n",
      "|     4.0| 10|2009-05-11 10:26:10| jquery|   dcostalis|Jquery is my new ...|\n",
      "|     4.0| 11|2009-05-11 10:27:15|twitter|     PJ_King|       Loves twitter|\n",
      "|     4.0| 12|2009-05-11 10:29:20|  obama| mandanicole|how can you not l...|\n",
      "+--------+---+-------------------+-------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- polarity: float (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+-----------------+-------+--------------------+\n",
      "|summary|          polarity|                id|            query|   user|                text|\n",
      "+-------+------------------+------------------+-----------------+-------+--------------------+\n",
      "|  count|               498|               498|              498|    498|                 498|\n",
      "|   mean|2.0200803212851404|1867.2269076305222|             46.0|   null|                null|\n",
      "| stddev|1.6996858490577658| 2834.891681137318|5.163977794943222|   null|                null|\n",
      "|    min|               0.0|                 3| \"\"\"booz allen\"\"\"| 5x1llz|\"\"\"The Republican...|\n",
      "|    25%|               0.0|               388|             40.0|   null|                null|\n",
      "|    50%|               2.0|              1013|             50.0|   null|                null|\n",
      "|    75%|               4.0|              2367|             50.0|   null|                null|\n",
      "|    max|               4.0|             14076|          yankees|zedomax|zomg!!! I have a ...|\n",
      "+-------+------------------+------------------+-----------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.tomeParserPolicy=LEGACY\")\n",
    "\n",
    "schema_ddl =  \"polarity Float, id LONG, date TIMESTAMP, query STRING, user STRING, text STRING\"\n",
    "spark_reader = spark.read.schema(schema_ddl)\n",
    "\n",
    "simple_date_format = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "RAW_PATH = \"./testdata.manual.2009.06.14.csv\"\n",
    "raw_data = spark_reader.csv(RAW_PATH, timestampFormat=simple_date_format)\n",
    "raw_data.show(10)\n",
    "raw_data.printSchema()\n",
    "\n",
    "raw_data.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users_mentioned</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@stellargirl]</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[@kenburbary]</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[@mikefish]</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[@richardebaker]</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[]</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[]</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[@Karoli]</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[]</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[]</td>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[@vincentx24x]</td>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[]</td>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[]</td>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[]</td>\n",
       "      <td>i love lebron. http://bit.ly/PdHur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[@ludajuice]</td>\n",
       "      <td>@ludajuice Lebron is a Beast, but I'm still ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[@Pmillzz]</td>\n",
       "      <td>@Pmillzz lebron IS THE BOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[@sketchbug]</td>\n",
       "      <td>@sketchbug Lebron is a hometown hero to me, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[]</td>\n",
       "      <td>lebron and zydrunas are such an awesome duo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[@wordwhizkid]</td>\n",
       "      <td>@wordwhizkid Lebron is a beast... nobody in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[]</td>\n",
       "      <td>downloading apps for my iphone! So much fun :-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[]</td>\n",
       "      <td>good news, just had a call from the Visa offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[@biz, @fredwilson]</td>\n",
       "      <td>http://twurl.nl/epkr4b - awesome come back fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[]</td>\n",
       "      <td>In montreal for a long weekend of R&amp;amp;R. Muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[]</td>\n",
       "      <td>Booz Allen Hamilton has a bad ass homegrown so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[]</td>\n",
       "      <td>[#MLUC09] Customer Innovation Award Winner: Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[@SoChi2]</td>\n",
       "      <td>@SoChi2 I current use the Nikon D90 and love i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[]</td>\n",
       "      <td>need suggestions for a good IR filter for my c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[@surfit]</td>\n",
       "      <td>@surfit: I just checked my google for my busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[@phyreman9, @KimbleT]</td>\n",
       "      <td>@phyreman9 Google is always a good place to lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[]</td>\n",
       "      <td>Played with an android google phone. The slide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[]</td>\n",
       "      <td>US planning to resume the military tribunals a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           users_mentioned                                               text\n",
       "0           [@stellargirl]  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1                       []  Reading my kindle2...  Love it... Lee childs i...\n",
       "2                       []  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3            [@kenburbary]  @kenburbary You'll love your Kindle2. I've had...\n",
       "4              [@mikefish]  @mikefish  Fair enough. But i have the Kindle2...\n",
       "5         [@richardebaker]  @richardebaker no. it is too big. I'm quite ha...\n",
       "6                       []  Fuck this economy. I hate aig and their non lo...\n",
       "7                       []                      Jquery is my new best friend.\n",
       "8                       []                                      Loves twitter\n",
       "9                       []  how can you not love Obama? he makes jokes abo...\n",
       "10                      []  Check this video out -- President Obama at the...\n",
       "11               [@Karoli]  @Karoli I firmly believe that Obama/Pelosi hav...\n",
       "12                      []  House Correspondents dinner was last night who...\n",
       "13                      []  Watchin Espn..Jus seen this new Nike Commerica...\n",
       "14          [@vincentx24x]  dear nike, stop with the flywire. that shit is...\n",
       "15                      []  #lebron best athlete of our generation, if not...\n",
       "16                      []  I was talking to this guy last night and he wa...\n",
       "17                      []                 i love lebron. http://bit.ly/PdHur\n",
       "18            [@ludajuice]  @ludajuice Lebron is a Beast, but I'm still ch...\n",
       "19              [@Pmillzz]                        @Pmillzz lebron IS THE BOSS\n",
       "20            [@sketchbug]  @sketchbug Lebron is a hometown hero to me, lo...\n",
       "21                      []        lebron and zydrunas are such an awesome duo\n",
       "22          [@wordwhizkid]  @wordwhizkid Lebron is a beast... nobody in th...\n",
       "23                      []  downloading apps for my iphone! So much fun :-...\n",
       "24                      []  good news, just had a call from the Visa offic...\n",
       "25     [@biz, @fredwilson]  http://twurl.nl/epkr4b - awesome come back fro...\n",
       "26                      []  In montreal for a long weekend of R&amp;R. Muc...\n",
       "27                      []  Booz Allen Hamilton has a bad ass homegrown so...\n",
       "28                      []  [#MLUC09] Customer Innovation Award Winner: Bo...\n",
       "29               [@SoChi2]  @SoChi2 I current use the Nikon D90 and love i...\n",
       "30                      []  need suggestions for a good IR filter for my c...\n",
       "31               [@surfit]  @surfit: I just checked my google for my busin...\n",
       "32  [@phyreman9, @KimbleT]  @phyreman9 Google is always a good place to lo...\n",
       "33                      []  Played with an android google phone. The slide...\n",
       "34                      []  US planning to resume the military tribunals a..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_regex = r\"(@\\w{1,15})\"\n",
    "\n",
    "raw_data.select(\n",
    "    f.array_remove(\n",
    "        f.array(\n",
    "            f.regexp_extract(f.col(\"text\"),user_regex,1),\n",
    "            f.regexp_extract(\n",
    "                f.col(\"text\"),\"\".join([f\"{user_regex}.*?\" for i in range(0,2)]), 2      \n",
    "            ),\n",
    "            f.regexp_extract(\n",
    "                f.col(\"text\"),\"\".join([f\"{user_regex}.*?\" for i in range(0,3)]), 3      \n",
    "            ),\n",
    "            f.regexp_extract(\n",
    "                f.col(\"text\"),\"\".join([f\"{user_regex}.*?\" for i in range(0,4)]), 4      \n",
    "            ),\n",
    "            f.regexp_extract(\n",
    "                f.col(\"text\"),\"\".join([f\"{user_regex}.*?\" for i in range(0,5)]), 5      \n",
    "            ),\n",
    "            f.regexp_extract(\n",
    "                f.col(\"text\"),\"\".join([f\"{user_regex}.*?\" for i in range(0,6)]), 6      \n",
    "            ),\n",
    "        ),\n",
    "        \"\",\n",
    "    ).alias(\"users_mentioned\"),\n",
    "    \"text\",\n",
    ").toPandas().head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I loooooooovvvvvveee my Kindle2. Not that the...</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You'll love your Kindle2. I've had mine for a...</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fair enough. But i have the Kindle2 and I th...</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no. it is too big. I'm quite happy with the K...</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Loves twitter</td>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I firmly believe that Obama/Pelosi have ZERO ...</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i love lebron. http://bit.ly/PdHur</td>\n",
       "      <td>i love lebron. http://bit.ly/PdHur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lebron is a Beast, but I'm still cheering 4 t...</td>\n",
       "      <td>@ludajuice Lebron is a Beast, but I'm still ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lebron IS THE BOSS</td>\n",
       "      <td>@Pmillzz lebron IS THE BOSS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0    I loooooooovvvvvveee my Kindle2. Not that the...   \n",
       "1   Reading my kindle2...  Love it... Lee childs i...   \n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...   \n",
       "3    You'll love your Kindle2. I've had mine for a...   \n",
       "4     Fair enough. But i have the Kindle2 and I th...   \n",
       "5    no. it is too big. I'm quite happy with the K...   \n",
       "6   Fuck this economy. I hate aig and their non lo...   \n",
       "7                       Jquery is my new best friend.   \n",
       "8                                       Loves twitter   \n",
       "9   how can you not love Obama? he makes jokes abo...   \n",
       "10  Check this video out -- President Obama at the...   \n",
       "11   I firmly believe that Obama/Pelosi have ZERO ...   \n",
       "12  House Correspondents dinner was last night who...   \n",
       "13  Watchin Espn..Jus seen this new Nike Commerica...   \n",
       "14  dear nike, stop with the flywire. that shit is...   \n",
       "15  #lebron best athlete of our generation, if not...   \n",
       "16  I was talking to this guy last night and he wa...   \n",
       "17                 i love lebron. http://bit.ly/PdHur   \n",
       "18   Lebron is a Beast, but I'm still cheering 4 t...   \n",
       "19                                 lebron IS THE BOSS   \n",
       "\n",
       "                                        original_text  \n",
       "0   @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1   Reading my kindle2...  Love it... Lee childs i...  \n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3   @kenburbary You'll love your Kindle2. I've had...  \n",
       "4   @mikefish  Fair enough. But i have the Kindle2...  \n",
       "5   @richardebaker no. it is too big. I'm quite ha...  \n",
       "6   Fuck this economy. I hate aig and their non lo...  \n",
       "7                       Jquery is my new best friend.  \n",
       "8                                       Loves twitter  \n",
       "9   how can you not love Obama? he makes jokes abo...  \n",
       "10  Check this video out -- President Obama at the...  \n",
       "11  @Karoli I firmly believe that Obama/Pelosi hav...  \n",
       "12  House Correspondents dinner was last night who...  \n",
       "13  Watchin Espn..Jus seen this new Nike Commerica...  \n",
       "14  dear nike, stop with the flywire. that shit is...  \n",
       "15  #lebron best athlete of our generation, if not...  \n",
       "16  I was talking to this guy last night and he wa...  \n",
       "17                 i love lebron. http://bit.ly/PdHur  \n",
       "18  @ludajuice Lebron is a Beast, but I'm still ch...  \n",
       "19                        @Pmillzz lebron IS THE BOSS  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.select(\n",
    "    f.regexp_replace(f.col(\"text\"), user_regex, \"\").alias(\"text\"),\n",
    "    f.col(\"text\").alias(\"original_text\"),\n",
    ").toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-7c7ff62bf027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhashtag_replace_regrex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"#(\\w{1,})\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m raw_data.select(\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashtag_replace_regrex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"$1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hashtags\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m ).toPandas().head(20)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "hashtag_replace_regrex = \"#(\\w{1,})\"\n",
    "\n",
    "_.select(f.regexp_replace(f.col(\"text\"), hashtag_replace_regrex, \"$1\"), \"hashtags\").show(35,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`hashtags`' given input columns: [date, id, polarity, query, text, user];;\n'Project [regexp_replace(text#1613, #(\\w{1,}), $1) AS regexp_replace(text, #(\\w{1,}), $1)#2408, 'hashtags]\n+- Relation[polarity#1608,id#1609L,date#1610,query#1611,user#1612,text#1613] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-7981d1a62b23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhashtag_replace_regrex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"#(\\w{1,})\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m raw_data.select(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashtag_replace_regrex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"$1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"hashtags\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ).show(35,100)\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \"\"\"\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`hashtags`' given input columns: [date, id, polarity, query, text, user];;\n'Project [regexp_replace(text#1613, #(\\w{1,}), $1) AS regexp_replace(text, #(\\w{1,}), $1)#2408, 'hashtags]\n+- Relation[polarity#1608,id#1609L,date#1610,query#1611,user#1612,text#1613] csv\n"
     ]
    }
   ],
   "source": [
    "hashtag_replace_regrex = \"#(\\w{1,})\"\n",
    "raw_data.select(\n",
    "    f.regexp_replace(f.col(\"text\"), hashtag_replace_regrex, \"$1\"),\n",
    "    \"hashtags\"\n",
    ").show(35,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "email_regex = r\"/\\S+@\\S+\\.\\S+/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_no_email</th>\n",
       "      <th>text_no_url</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Loves twitter</td>\n",
       "      <td>Loves twitter</td>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i love lebron. http://bit.ly/PdHur</td>\n",
       "      <td>i love lebron.</td>\n",
       "      <td>i love lebron. http://bit.ly/PdHur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@ludajuice Lebron is a Beast, but I'm still ch...</td>\n",
       "      <td>@ludajuice Lebron is a Beast, but I'm still ch...</td>\n",
       "      <td>@ludajuice Lebron is a Beast, but I'm still ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@Pmillzz lebron IS THE BOSS</td>\n",
       "      <td>@Pmillzz lebron IS THE BOSS</td>\n",
       "      <td>@Pmillzz lebron IS THE BOSS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_no_email  \\\n",
       "0   @stellargirl I loooooooovvvvvveee my Kindle2. ...   \n",
       "1   Reading my kindle2...  Love it... Lee childs i...   \n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...   \n",
       "3   @kenburbary You'll love your Kindle2. I've had...   \n",
       "4   @mikefish  Fair enough. But i have the Kindle2...   \n",
       "5   @richardebaker no. it is too big. I'm quite ha...   \n",
       "6   Fuck this economy. I hate aig and their non lo...   \n",
       "7                       Jquery is my new best friend.   \n",
       "8                                       Loves twitter   \n",
       "9   how can you not love Obama? he makes jokes abo...   \n",
       "10  Check this video out -- President Obama at the...   \n",
       "11  @Karoli I firmly believe that Obama/Pelosi hav...   \n",
       "12  House Correspondents dinner was last night who...   \n",
       "13  Watchin Espn..Jus seen this new Nike Commerica...   \n",
       "14  dear nike, stop with the flywire. that shit is...   \n",
       "15  #lebron best athlete of our generation, if not...   \n",
       "16  I was talking to this guy last night and he wa...   \n",
       "17                 i love lebron. http://bit.ly/PdHur   \n",
       "18  @ludajuice Lebron is a Beast, but I'm still ch...   \n",
       "19                        @Pmillzz lebron IS THE BOSS   \n",
       "\n",
       "                                          text_no_url  \\\n",
       "0   @stellargirl I loooooooovvvvvveee my Kindle2. ...   \n",
       "1   Reading my kindle2...  Love it... Lee childs i...   \n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...   \n",
       "3   @kenburbary You'll love your Kindle2. I've had...   \n",
       "4   @mikefish  Fair enough. But i have the Kindle2...   \n",
       "5   @richardebaker no. it is too big. I'm quite ha...   \n",
       "6   Fuck this economy. I hate aig and their non lo...   \n",
       "7                       Jquery is my new best friend.   \n",
       "8                                       Loves twitter   \n",
       "9   how can you not love Obama? he makes jokes abo...   \n",
       "10  Check this video out -- President Obama at the...   \n",
       "11  @Karoli I firmly believe that Obama/Pelosi hav...   \n",
       "12  House Correspondents dinner was last night who...   \n",
       "13  Watchin Espn..Jus seen this new Nike Commerica...   \n",
       "14  dear nike, stop with the flywire. that shit is...   \n",
       "15  #lebron best athlete of our generation, if not...   \n",
       "16  I was talking to this guy last night and he wa...   \n",
       "17                                    i love lebron.    \n",
       "18  @ludajuice Lebron is a Beast, but I'm still ch...   \n",
       "19                        @Pmillzz lebron IS THE BOSS   \n",
       "\n",
       "                                        original_text  \n",
       "0   @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1   Reading my kindle2...  Love it... Lee childs i...  \n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3   @kenburbary You'll love your Kindle2. I've had...  \n",
       "4   @mikefish  Fair enough. But i have the Kindle2...  \n",
       "5   @richardebaker no. it is too big. I'm quite ha...  \n",
       "6   Fuck this economy. I hate aig and their non lo...  \n",
       "7                       Jquery is my new best friend.  \n",
       "8                                       Loves twitter  \n",
       "9   how can you not love Obama? he makes jokes abo...  \n",
       "10  Check this video out -- President Obama at the...  \n",
       "11  @Karoli I firmly believe that Obama/Pelosi hav...  \n",
       "12  House Correspondents dinner was last night who...  \n",
       "13  Watchin Espn..Jus seen this new Nike Commerica...  \n",
       "14  dear nike, stop with the flywire. that shit is...  \n",
       "15  #lebron best athlete of our generation, if not...  \n",
       "16  I was talking to this guy last night and he wa...  \n",
       "17                 i love lebron. http://bit.ly/PdHur  \n",
       "18  @ludajuice Lebron is a Beast, but I'm still ch...  \n",
       "19                        @Pmillzz lebron IS THE BOSS  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.select(\n",
    "    f.regexp_replace(f.col(\"text\"), email_regex,\"\").alias(\"text_no_email\"),\n",
    "    f.regexp_replace(f.col(\"text\"), url_regex,\"\").alias(\"text_no_url\"),\n",
    "    f.col(\"text\").alias(\"original_text\"),\n",
    ").toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_regex = r\"(@\\w{1,15})\"\n",
    "hashtag_replace_regex= \"#(\\w{1,})\"\n",
    "url_regex = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "email_regex = r\"/\\S+@\\S+\\.\\S+/\"\n",
    "spectial_character = \"[^A-Za-z0-9]\"\n",
    "from pyspark.sql.functions import trim\n",
    "def cleaning(df):  \n",
    "    df = df.withColumn(\"text\", f.regexp_replace(f.col(\"text\"),user_regex, \"\"))\n",
    "    df = df.withColumn(\"text\", f.regexp_replace(f.col(\"text\"),hashtag_replace_regex, \"\"))\n",
    "    df = df.withColumn(\"text\", f.regexp_replace(f.col(\"text\"),email_regex, \"\"))\n",
    "    df = df.withColumn(\"text\", f.regexp_replace(f.col(\"text\"),url_regex, \"\"))\n",
    "    df = df.withColumn(\"text\", f.regexp_replace(f.col(\"text\"),spectial_character, \" \"))\n",
    "    df = df.withColumn(\"text\", f.trim(f.col(\"text\")))  \n",
    "    df = df.withColumn(\"text\", f.regexp_replace(f.col(\"text\"),\"  *\", \" \"))    \n",
    "    df = df.withColumn(\"text\", f.lower(f.col(\"text\"))) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
